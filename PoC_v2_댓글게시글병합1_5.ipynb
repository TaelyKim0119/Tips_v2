{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>video_publishedAt</th>\n",
       "      <th>comment_publishedAt</th>\n",
       "      <th>hours_after</th>\n",
       "      <th>time_diff_display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9GukLzTRkkM</td>\n",
       "      <td>2025-02-27 11:45:38+00:00</td>\n",
       "      <td>2025-03-09 07:28:57+00:00</td>\n",
       "      <td>235.721944</td>\n",
       "      <td>9ì¼ 19ì‹œê°„ 43ë¶„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9GukLzTRkkM</td>\n",
       "      <td>2025-02-27 11:45:38+00:00</td>\n",
       "      <td>2025-03-08 11:10:03+00:00</td>\n",
       "      <td>215.406944</td>\n",
       "      <td>8ì¼ 23ì‹œê°„ 24ë¶„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9GukLzTRkkM</td>\n",
       "      <td>2025-02-27 11:45:38+00:00</td>\n",
       "      <td>2025-03-05 11:22:08+00:00</td>\n",
       "      <td>143.608333</td>\n",
       "      <td>5ì¼ 23ì‹œê°„ 36ë¶„</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId         video_publishedAt       comment_publishedAt  \\\n",
       "0  9GukLzTRkkM 2025-02-27 11:45:38+00:00 2025-03-09 07:28:57+00:00   \n",
       "1  9GukLzTRkkM 2025-02-27 11:45:38+00:00 2025-03-08 11:10:03+00:00   \n",
       "2  9GukLzTRkkM 2025-02-27 11:45:38+00:00 2025-03-05 11:22:08+00:00   \n",
       "\n",
       "   hours_after time_diff_display  \n",
       "0   235.721944       9ì¼ 19ì‹œê°„ 43ë¶„  \n",
       "1   215.406944       8ì¼ 23ì‹œê°„ 24ë¶„  \n",
       "2   143.608333       5ì¼ 23ì‹œê°„ 36ë¶„  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import timedelta\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 1ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# # ------------------------------------------------------------\n",
    "# comments = pd.read_excel(\n",
    "#     r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\GSë¦¬í…Œì¼_ì •ë³´ìœ ì¶œ_comments_GPT_labeled.xlsx\"\n",
    "# )\n",
    "# posts = pd.read_excel(\n",
    "#     r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\GS_posts_GPT_labeled.xlsx\"\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 2ï¸âƒ£ ì‹œê°„ ì •ë¦¬\n",
    "# # ------------------------------------------------------------\n",
    "# comments[\"publishedAt\"] = pd.to_datetime(comments[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "# posts[\"publishedAt\"] = pd.to_datetime(posts[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 3ï¸âƒ£ ê²Œì‹œê¸€ ê¸°ì¤€ìœ¼ë¡œ ëŒ“ê¸€ ë³‘í•© (LEFT JOIN)\n",
    "# # ------------------------------------------------------------\n",
    "# merged = pd.merge(\n",
    "#     posts,\n",
    "#     comments[[\"videoId\", \"publishedAt\", \"gpt_sentiment\"]],\n",
    "#     on=\"videoId\",\n",
    "#     how=\"left\",\n",
    "#     suffixes=(\"_x\", \"_y\")\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 4ï¸âƒ£ ëŒ“ê¸€ ì‘ì„± ì‹œì ì´ ê²Œì‹œê¸€ ì—…ë¡œë“œ í›„ ëª‡ ì‹œê°„ ë’¤ì¸ì§€ ê³„ì‚°\n",
    "# # ------------------------------------------------------------\n",
    "# merged[\"hours_after\"] = (\n",
    "#     (merged[\"publishedAt_y\"] - merged[\"publishedAt_x\"])\n",
    "#     .dt.total_seconds() / 3600\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 5ï¸âƒ£ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í¬ë§·ìœ¼ë¡œ ë³€í™˜ (ì¼, ì‹œ, ë¶„)\n",
    "# # ------------------------------------------------------------\n",
    "# def format_time_diff(hours):\n",
    "#     if pd.isna(hours):\n",
    "#         return \"-\"\n",
    "#     days = int(hours // 24)\n",
    "#     remain_hours = int(hours % 24)\n",
    "#     minutes = int((hours * 60) % 60)\n",
    "#     return f\"{days}ì¼ {remain_hours}ì‹œê°„ {minutes}ë¶„\"\n",
    "\n",
    "# merged[\"time_diff_display\"] = merged[\"hours_after\"].apply(format_time_diff)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 6ï¸âƒ£ ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬\n",
    "# # ------------------------------------------------------------\n",
    "# merged.rename(\n",
    "#     columns={\n",
    "#         \"publishedAt_x\": \"video_publishedAt\",   # ê²Œì‹œê¸€ ì—…ë¡œë“œ ì‹œê°\n",
    "#         \"publishedAt_y\": \"comment_publishedAt\"  # ëŒ“ê¸€ ì‘ì„± ì‹œê°\n",
    "#     },\n",
    "#     inplace=True\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 7ï¸âƒ£ í™•ì¸\n",
    "# # ------------------------------------------------------------\n",
    "# display(\n",
    "#     merged[[\"videoId\", \"video_publishedAt\", \"comment_publishedAt\", \"hours_after\", \"time_diff_display\"]].head(3)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321385dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³‘í•© íŒŒì¼ ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“ ê²½ë¡œ: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\GS_merged_result_20251107_1510.xlsx\n",
      "ğŸ“Š ì €ì¥ëœ í–‰ ê°œìˆ˜: 200ê°œ\n",
      "ğŸ•“ ì €ì¥ ì‹œê°: 20251107_1510\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 1ï¸âƒ£ íƒ€ì„ì¡´ ì œê±° (tz-aware â†’ naive)\n",
    "# # ----------------------------------------\n",
    "# for col in [\"video_publishedAt\", \"comment_publishedAt\"]:\n",
    "#     if col in merged.columns and pd.api.types.is_datetime64_any_dtype(merged[col]):\n",
    "#         merged[col] = merged[col].dt.tz_localize(None)\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 2ï¸âƒ£ ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±° (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì•ˆì „í•˜ê²Œ drop)\n",
    "# # ----------------------------------------\n",
    "# cols_to_drop = [\n",
    "#     \"title\", \"description\", \"channelId\", \"url\",\n",
    "#     \"gpt_post_confidence\", \"comment_publishedAt\"\n",
    "# ]\n",
    "# existing = [c for c in cols_to_drop if c in merged.columns]\n",
    "# merged = merged.drop(columns=existing)\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 3ï¸âƒ£ ì¶œë ¥ ê²½ë¡œ ìë™ ìƒì„± (ë‚ ì§œÂ·ì‹œê°„ í¬í•¨)\n",
    "# # ----------------------------------------\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "# output_dir = r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# output_path = os.path.join(output_dir, f\"GS_merged_result_{timestamp}.xlsx\")\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 4ï¸âƒ£ Excelë¡œ ì €ì¥ (ì¸ë±ìŠ¤ ì œì™¸)\n",
    "# # ----------------------------------------\n",
    "# merged.to_excel(output_path, index=False)\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 5ï¸âƒ£ ë¡œê·¸ ì¶œë ¥\n",
    "# # ----------------------------------------\n",
    "# print(\"âœ… ë³‘í•© íŒŒì¼ ì €ì¥ ì™„ë£Œ\")\n",
    "# print(f\"ğŸ“ ê²½ë¡œ: {output_path}\")\n",
    "# print(f\"ğŸ“Š ì €ì¥ëœ í–‰ ê°œìˆ˜: {len(merged):,}ê°œ\")\n",
    "# print(f\"ğŸ•“ ì €ì¥ ì‹œê°: {timestamp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb6bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>post_s</th>\n",
       "      <th>cmt_s_mean</th>\n",
       "      <th>abs_sent_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0jmI8cbpUY4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.722222</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2fCVoYX1dMo</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId  post_s  cmt_s_mean  abs_sent_gap\n",
       "0  0jmI8cbpUY4      -1   -0.722222      0.277778\n",
       "1  2fCVoYX1dMo      -1   -1.000000      0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=merged\n",
    "# # ê°ì • ë§¤í•‘\n",
    "# # ê°ì • ë§¤í•‘\n",
    "# MAP = {\"ê¸ì •\": 1, \"ì¤‘ë¦½\": 0, \"ë¶€ì •\": -1}\n",
    "\n",
    "# df[\"post_s\"] = df[\"gpt_post_sentiment\"].map(MAP)\n",
    "# df[\"cmt_s\"]  = df[\"gpt_sentiment\"].map(MAP)\n",
    "\n",
    "# # videoId ë‹¨ìœ„ë¡œ ëŒ“ê¸€ í‰ê·  ê°ì • ê³„ì‚°\n",
    "# agg = (\n",
    "#     df.groupby(\"videoId\")\n",
    "#       .agg(post_s=(\"post_s\", \"first\"),\n",
    "#            cmt_s_mean=(\"cmt_s\", \"mean\"))\n",
    "#       .reset_index()\n",
    "# )\n",
    "\n",
    "# # ì ˆëŒ€ ê´´ë¦¬ë§Œ ê³„ì‚° (0~2 ë²”ìœ„)\n",
    "# agg[\"abs_sent_gap\"] = (agg[\"post_s\"] - agg[\"cmt_s_mean\"]).abs()\n",
    "\n",
    "# # ê²°ê³¼: ì˜ìƒë³„ ê´´ë¦¬ ì •ë„ë§Œ ì¶œë ¥\n",
    "# result = agg[[\"videoId\", \"abs_sent_gap\"]]\n",
    "\n",
    "# agg.head(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244075cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\speec\\AppData\\Local\\Temp\\ipykernel_18160\\3392540805.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  align_df = df.groupby(\"videoId\").apply(alignment_features).reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>aligned_cnt_3h</th>\n",
       "      <th>aligned_cnt_6h</th>\n",
       "      <th>aligned_cnt_9h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0jmI8cbpUY4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2fCVoYX1dMo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId  aligned_cnt_3h  aligned_cnt_6h  aligned_cnt_9h\n",
       "0  0jmI8cbpUY4               7              10              12\n",
       "1  2fCVoYX1dMo               0               0               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def calc_alignment(df, t):\n",
    "#     # tì‹œê°„ ì´ë‚´ì˜ ëŒ“ê¸€ë§Œ ì¶”ì¶œ\n",
    "#     sub = df[df[\"hours_after\"] <= t]\n",
    "#     total = len(sub)\n",
    "#     if total == 0:\n",
    "#         return 0  # ëŒ“ê¸€ì´ ì—†ì„ ë•ŒëŠ” 0ê°œë¡œ ì²˜ë¦¬\n",
    "    \n",
    "#     # ì˜ìƒ ê°ì •ê³¼ ë™ì¼í•œ ëŒ“ê¸€ ê°œìˆ˜ ê³„ì‚°\n",
    "#     aligned = (sub[\"gpt_sentiment\"] == sub[\"gpt_post_sentiment\"]).sum()\n",
    "#     return aligned  # â† ë¹„ìœ¨ì´ ì•„ë‹ˆë¼ ê°œìˆ˜ ë°˜í™˜\n",
    "\n",
    "# def alignment_features(group):\n",
    "#     return pd.Series({\n",
    "#         \"aligned_cnt_3h\": calc_alignment(group, 3),\n",
    "#         \"aligned_cnt_6h\": calc_alignment(group, 6),\n",
    "#         \"aligned_cnt_9h\": calc_alignment(group, 9)\n",
    "#     })\n",
    "\n",
    "# # ì˜ìƒ(videoId)ë³„ë¡œ ì§‘ê³„\n",
    "# align_df = df.groupby(\"videoId\").apply(alignment_features).reset_index()\n",
    "\n",
    "# align_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e4a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>aligned_cnt_3h</th>\n",
       "      <th>aligned_cnt_6h</th>\n",
       "      <th>aligned_cnt_9h</th>\n",
       "      <th>post_s</th>\n",
       "      <th>cmt_s_mean</th>\n",
       "      <th>abs_sent_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0jmI8cbpUY4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.722222</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2fCVoYX1dMo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId  aligned_cnt_3h  aligned_cnt_6h  aligned_cnt_9h  post_s  \\\n",
       "0  0jmI8cbpUY4               7              10              12      -1   \n",
       "1  2fCVoYX1dMo               0               0               0      -1   \n",
       "\n",
       "   cmt_s_mean  abs_sent_gap  \n",
       "0   -0.722222      0.277778  \n",
       "1   -1.000000      0.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_df = pd.merge(align_df, agg, on=\"videoId\", how=\"inner\")\n",
    "# merged_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c4370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>subscriberCount</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>gpt_post_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9GukLzTRkkM</td>\n",
       "      <td>JTBC News</td>\n",
       "      <td>2025-02-27 11:45:38+00:00</td>\n",
       "      <td>13151</td>\n",
       "      <td>228</td>\n",
       "      <td>120</td>\n",
       "      <td>4750000</td>\n",
       "      <td>ì£¼ì†Œì— í†µê´€ë²ˆí˜¸ê¹Œì§€â€¦GSë¦¬í…Œì¼, 158ë§Œëª… ì •ë³´ ìœ ì¶œ í™•ì¸ / JTBC ë‰´ìŠ¤ë£¸ ì§€ë‚œ...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qcQjIfuls3c</td>\n",
       "      <td>SBS ë‰´ìŠ¤</td>\n",
       "      <td>2025-01-06 09:57:34+00:00</td>\n",
       "      <td>6297</td>\n",
       "      <td>68</td>\n",
       "      <td>42</td>\n",
       "      <td>5080000</td>\n",
       "      <td>GSë¦¬í…Œì¼ ê³ ê° ì •ë³´ 9ë§Œì—¬ ê±´ í•´í‚¹ í”¼í•´â€¦3ì¼ ë§Œì— ì•Œë ¸ë‹¤ / SBS / #Dë¦¬í¬...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId channelTitle               publishedAt  viewCount  likeCount  \\\n",
       "0  9GukLzTRkkM    JTBC News 2025-02-27 11:45:38+00:00      13151        228   \n",
       "1  qcQjIfuls3c       SBS ë‰´ìŠ¤ 2025-01-06 09:57:34+00:00       6297         68   \n",
       "\n",
       "   commentCount  subscriberCount  \\\n",
       "0           120          4750000   \n",
       "1            42          5080000   \n",
       "\n",
       "                                            text_raw gpt_post_sentiment  \n",
       "0  ì£¼ì†Œì— í†µê´€ë²ˆí˜¸ê¹Œì§€â€¦GSë¦¬í…Œì¼, 158ë§Œëª… ì •ë³´ ìœ ì¶œ í™•ì¸ / JTBC ë‰´ìŠ¤ë£¸ ì§€ë‚œ...                 ë¶€ì •  \n",
       "1  GSë¦¬í…Œì¼ ê³ ê° ì •ë³´ 9ë§Œì—¬ ê±´ í•´í‚¹ í”¼í•´â€¦3ì¼ ë§Œì— ì•Œë ¸ë‹¤ / SBS / #Dë¦¬í¬...                 ë¶€ì •  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# for col in [\"video_publishedAt\", \"comment_publishedAt\"]:\n",
    "#     if col in posts.columns and pd.api.types.is_datetime64_any_dtype(posts[col]):\n",
    "#         posts[col] = posts[col].dt.tz_localize(None)\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 2ï¸âƒ£ ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±° (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì•ˆì „í•˜ê²Œ drop)\n",
    "# # ----------------------------------------\n",
    "# cols_to_drop = [\n",
    "#     \"title\", \"description\", \"channelId\", \"url\",\n",
    "#     \"gpt_post_confidence\", \"comment_publishedAt\"\n",
    "# ]\n",
    "# existing = [c for c in cols_to_drop if c in posts.columns]\n",
    "# posts = posts.drop(columns=existing)\n",
    "\n",
    "# posts.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b660a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>subscriberCount</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>gpt_post_sentiment</th>\n",
       "      <th>aligned_cnt_3h</th>\n",
       "      <th>aligned_cnt_6h</th>\n",
       "      <th>aligned_cnt_9h</th>\n",
       "      <th>post_s</th>\n",
       "      <th>cmt_s_mean</th>\n",
       "      <th>abs_sent_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9GukLzTRkkM</td>\n",
       "      <td>JTBC News</td>\n",
       "      <td>2025-02-27 11:45:38+00:00</td>\n",
       "      <td>13151</td>\n",
       "      <td>228</td>\n",
       "      <td>120</td>\n",
       "      <td>4750000</td>\n",
       "      <td>ì£¼ì†Œì— í†µê´€ë²ˆí˜¸ê¹Œì§€â€¦GSë¦¬í…Œì¼, 158ë§Œëª… ì •ë³´ ìœ ì¶œ í™•ì¸ / JTBC ë‰´ìŠ¤ë£¸ ì§€ë‚œ...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.820000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qcQjIfuls3c</td>\n",
       "      <td>SBS ë‰´ìŠ¤</td>\n",
       "      <td>2025-01-06 09:57:34+00:00</td>\n",
       "      <td>6297</td>\n",
       "      <td>68</td>\n",
       "      <td>42</td>\n",
       "      <td>5080000</td>\n",
       "      <td>GSë¦¬í…Œì¼ ê³ ê° ì •ë³´ 9ë§Œì—¬ ê±´ í•´í‚¹ í”¼í•´â€¦3ì¼ ë§Œì— ì•Œë ¸ë‹¤ / SBS / #Dë¦¬í¬...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.783784</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId channelTitle               publishedAt  viewCount  likeCount  \\\n",
       "0  9GukLzTRkkM    JTBC News 2025-02-27 11:45:38+00:00      13151        228   \n",
       "1  qcQjIfuls3c       SBS ë‰´ìŠ¤ 2025-01-06 09:57:34+00:00       6297         68   \n",
       "\n",
       "   commentCount  subscriberCount  \\\n",
       "0           120          4750000   \n",
       "1            42          5080000   \n",
       "\n",
       "                                            text_raw gpt_post_sentiment  \\\n",
       "0  ì£¼ì†Œì— í†µê´€ë²ˆí˜¸ê¹Œì§€â€¦GSë¦¬í…Œì¼, 158ë§Œëª… ì •ë³´ ìœ ì¶œ í™•ì¸ / JTBC ë‰´ìŠ¤ë£¸ ì§€ë‚œ...                 ë¶€ì •   \n",
       "1  GSë¦¬í…Œì¼ ê³ ê° ì •ë³´ 9ë§Œì—¬ ê±´ í•´í‚¹ í”¼í•´â€¦3ì¼ ë§Œì— ì•Œë ¸ë‹¤ / SBS / #Dë¦¬í¬...                 ë¶€ì •   \n",
       "\n",
       "   aligned_cnt_3h  aligned_cnt_6h  aligned_cnt_9h  post_s  cmt_s_mean  \\\n",
       "0               7              11              21      -1   -0.820000   \n",
       "1              18              21              22      -1   -0.783784   \n",
       "\n",
       "   abs_sent_gap  \n",
       "0      0.180000  \n",
       "1      0.216216  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # posts, merged_df, align_df ëª¨ë‘ ë³‘í•© videoId ê¸°ì¤€ìœ¼ë¡œ\n",
    "\n",
    "# # ì¤‘ë³µ ì»¬ëŸ¼ì„ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ì •ë¦¬í•´ì„œ ë³‘í•©\n",
    "# final_df = posts.merge(merged_df, on=\"videoId\", how=\"inner\")\n",
    "\n",
    "# # align_df ì—ì„œ ì´ë¯¸ ë³‘í•©ëœ ì»¬ëŸ¼(merged_dfì™€ ë™ì¼í•œ ì»¬ëŸ¼ëª…)ì€ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ë§Œ ì‚¬ìš©\n",
    "# align_only_cols = [col for col in align_df.columns if col not in final_df.columns or col == \"videoId\"]\n",
    "\n",
    "# # 'videoId'ëŠ” ê¼­ ë“¤ì–´ê°€ì•¼ í•˜ë¯€ë¡œ í¬í•¨, ë‚˜ë¨¸ì§€ëŠ” ì¤‘ë³µ ì•„ë‹˜ë§Œ ë‚¨ê¹€\n",
    "# final_df = final_df.merge(align_df[align_only_cols], on=\"videoId\", how=\"inner\")\n",
    "\n",
    "# final_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a976d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì—‘ì…€ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# # âœ… 1ï¸âƒ£ ì‹œê°„ëŒ€(tz) ì œê±° í›„ ì €ì¥\n",
    "# final_df[\"publishedAt\"] = final_df[\"publishedAt\"].dt.tz_localize(None)\n",
    "\n",
    "# # âœ… 2ï¸âƒ£ ì—‘ì…€ë¡œ ì €ì¥\n",
    "# final_df.to_excel(\n",
    "#     r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\final_GS_result.xlsx\",\n",
    "#     index=False,\n",
    "#     engine=\"openpyxl\"\n",
    "# )\n",
    "\n",
    "# print(\"âœ… ì—‘ì…€ ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414272c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ GS ì²˜ë¦¬ ì¤‘...\n",
      "âœ… GS ì™„ë£Œ: ëŒ“ê¸€ 200ê±´ â†’ ê²Œì‹œê¸€ 22ê±´\n",
      "\n",
      "ğŸš€ KT ì²˜ë¦¬ ì¤‘...\n",
      "âœ… KT ì™„ë£Œ: ëŒ“ê¸€ 6,670ê±´ â†’ ê²Œì‹œê¸€ 349ê±´\n",
      "\n",
      "ğŸš€ ë¡¯ë° ì²˜ë¦¬ ì¤‘...\n",
      "âœ… ë¡¯ë° ì™„ë£Œ: ëŒ“ê¸€ 5,611ê±´ â†’ ê²Œì‹œê¸€ 500ê±´\n",
      "\n",
      "ğŸš€ SKT ì²˜ë¦¬ ì¤‘...\n",
      "âœ… SKT ì™„ë£Œ: ëŒ“ê¸€ 7,658ê±´ â†’ ê²Œì‹œê¸€ 494ê±´\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 1. ê¸°ë³¸ ì„¤ì •\n",
    "# ============================================================\n",
    "BASE_PATH = r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 2. í•¨ìˆ˜ ì •ì˜\n",
    "# ============================================================\n",
    "\n",
    "def load_data(issue):\n",
    "    posts = pd.read_excel(os.path.join(BASE_PATH, f\"{issue}_posts_GPT_labeled.xlsx\"))\n",
    "    comments = pd.read_excel(os.path.join(BASE_PATH, f\"{issue}_ëŒ“ê¸€ê°ì •ë¶„ì„_GPT_labeled.xlsx\"))\n",
    "    posts[\"publishedAt\"] = pd.to_datetime(posts[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "    comments[\"publishedAt\"] = pd.to_datetime(comments[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "    return posts, comments\n",
    "\n",
    "\n",
    "def merge_post_comment(posts, comments):\n",
    "    merged = pd.merge(\n",
    "        posts,\n",
    "        comments[[\"videoId\", \"publishedAt\", \"gpt_sentiment\"]],\n",
    "        on=\"videoId\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_post\", \"_comment\")\n",
    "    )\n",
    "    merged[\"hours_after\"] = (\n",
    "        (merged[\"publishedAt_comment\"] - merged[\"publishedAt_post\"]).dt.total_seconds() / 3600\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "\n",
    "def calc_alignment(merged):\n",
    "    def to_score(x):\n",
    "        if isinstance(x, str):\n",
    "            if \"ë¶€ì •\" in x:\n",
    "                return -1\n",
    "            elif \"ê¸ì •\" in x:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    merged[\"post_s\"] = merged[\"gpt_post_sentiment\"].apply(to_score)\n",
    "    merged[\"cmt_s\"] = merged[\"gpt_sentiment\"].apply(to_score)\n",
    "\n",
    "    grouped = (\n",
    "        merged.groupby(\"videoId\")\n",
    "        .agg(\n",
    "            channelTitle=(\"channelTitle\", \"first\"),\n",
    "            publishedAt=(\"publishedAt_post\", \"first\"),\n",
    "            viewCount=(\"viewCount\", \"first\"),\n",
    "            likeCount=(\"likeCount\", \"first\"),\n",
    "            commentCount=(\"commentCount\", \"first\"),\n",
    "            subscriberCount=(\"subscriberCount\", \"first\"),\n",
    "            text_raw=(\"text_raw\", \"first\"),\n",
    "            gpt_post_sentiment=(\"gpt_post_sentiment\", \"first\"),\n",
    "            post_s=(\"post_s\", \"first\"),\n",
    "            cmt_s_mean=(\"cmt_s\", \"mean\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    grouped[\"abs_sent_gap\"] = (grouped[\"post_s\"] - grouped[\"cmt_s_mean\"]).abs().round(2)\n",
    "\n",
    "    for h in [3, 6, 9]:\n",
    "        cond = (merged[\"hours_after\"] <= h) & (merged[\"post_s\"] == merged[\"cmt_s\"])\n",
    "        aligned = merged[cond].groupby(\"videoId\").size()\n",
    "        grouped[f\"aligned_cnt_{h}h\"] = grouped[\"videoId\"].map(aligned).fillna(0).astype(int)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 3. ì‹¤í–‰ ë£¨í”„\n",
    "# ============================================================\n",
    "for issue in ISSUES:\n",
    "    print(f\"\\nğŸš€ {issue} ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "    posts, comments = load_data(issue)\n",
    "    merged = merge_post_comment(posts, comments)\n",
    "    result = calc_alignment(merged)\n",
    "\n",
    "    # Excel ì €ì¥ ì „ timezone ì œê±°\n",
    "    for col in merged.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "        merged[col] = merged[col].dt.tz_localize(None)   # â† ì—¬ê¸° ë“¤ì—¬ì“°ê¸° í•„ìˆ˜!\n",
    "\n",
    "    for col in result.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "        result[col] = result[col].dt.tz_localize(None)   # â† ì—¬ê¸°ë„ ë“¤ì—¬ì“°ê¸° í•„ìˆ˜!\n",
    "\n",
    "    # ì €ì¥\n",
    "    merged.to_excel(os.path.join(BASE_PATH, f\"{issue}_merged_timegap.xlsx\"), index=False, engine=\"openpyxl\")\n",
    "    result.to_excel(os.path.join(BASE_PATH, f\"{issue}_final_post_level.xlsx\"), index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} ì™„ë£Œ: ëŒ“ê¸€ {len(merged):,}ê±´ â†’ ê²Œì‹œê¸€ {len(result):,}ê±´\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ ì²˜ë¦¬ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd7ca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS ì™„ë£Œ â€” ê²Œì‹œê¸€ 186ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\GS_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ GS ì´ìŠˆ ëŒ“ê¸€ ì´ 593ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ… KT ì™„ë£Œ â€” ê²Œì‹œê¸€ 436ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\KT_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ KT ì´ìŠˆ ëŒ“ê¸€ ì´ 2,248ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ… ë¡¯ë° ì™„ë£Œ â€” ê²Œì‹œê¸€ 340ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\ë¡¯ë°_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ ë¡¯ë° ì´ìŠˆ ëŒ“ê¸€ ì´ 2,287ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ… SKT ì™„ë£Œ â€” ê²Œì‹œê¸€ 881ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\SKT_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ SKT ì´ìŠˆ ëŒ“ê¸€ ì´ 15,388ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ ê²Œì‹œê¸€ì— ëŒ“ê¸€ìˆ˜ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê²½ë¡œ ë° ì´ìŠˆ ëª©ë¡\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ëŒ“ê¸€ ìˆ˜ë¥¼ ê²Œì‹œê¸€ì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ + ì „ì²´ ëŒ“ê¸€ ê°œìˆ˜ ì¶œë ¥\n",
    "# ------------------------------------------------------------\n",
    "def add_comment_count_to_posts(issue):\n",
    "    \"\"\"ëŒ“ê¸€ ìˆ˜ë¥¼ ê²Œì‹œê¸€ íŒŒì¼ì— ì¶”ê°€í•œ ë’¤ ì €ì¥í•˜ë©°, ì „ì²´ ëŒ“ê¸€ ìˆ˜ë„ ì¶œë ¥\"\"\"\n",
    "    # íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    post_path = os.path.join(BASE_PATH, f\"{issue}_ë‰´ìŠ¤ê°ì •ë¶„ì„_GPT_labeled.xlsx\")\n",
    "    cmt_path = os.path.join(BASE_PATH, f\"{issue}_ëŒ“ê¸€ê°ì •ë¶„ì„_GPT_labeled.xlsx\")\n",
    "\n",
    "    posts = pd.read_excel(post_path)\n",
    "    comments = pd.read_excel(cmt_path)\n",
    "\n",
    "    # ë³‘í•© í‚¤ í†µì¼ (URL ê¸°ì¤€)\n",
    "    posts[\"videoId\"] = posts[\"url\"]\n",
    "    comments[\"videoId\"] = comments[\"url\"]\n",
    "\n",
    "    # âœ… ëŒ“ê¸€ ìˆ˜ ê³„ì‚° (videoId ê¸°ì¤€)\n",
    "    comment_counts = comments.groupby(\"videoId\").size().rename(\"comment_cnt\").reset_index()\n",
    "\n",
    "    # âœ… ê²Œì‹œê¸€ì— ëŒ“ê¸€ìˆ˜ ì¶”ê°€ (ì—†ìœ¼ë©´ 0)\n",
    "    posts = posts.merge(comment_counts, on=\"videoId\", how=\"left\").fillna({\"comment_cnt\": 0})\n",
    "    posts[\"comment_cnt\"] = posts[\"comment_cnt\"].astype(int)\n",
    "\n",
    "    # âœ… ì „ì²´ ëŒ“ê¸€ ìˆ˜ ì¹´ìš´íŠ¸\n",
    "    total_comments = comments.shape[0]\n",
    "\n",
    "    # âœ… ê²°ê³¼ ì €ì¥\n",
    "    output_path = os.path.join(BASE_PATH, f\"{issue}_posts_with_comment_cnt.xlsx\")\n",
    "    posts.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} ì™„ë£Œ â€” ê²Œì‹œê¸€ {len(posts)}ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\")\n",
    "    print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {output_path}\")\n",
    "    print(f\"ğŸ’¬ {issue} ì´ìŠˆ ëŒ“ê¸€ ì´ {total_comments:,}ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ ì‹¤í–‰\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    add_comment_count_to_posts(issue)\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ ê²Œì‹œê¸€ì— ëŒ“ê¸€ìˆ˜ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ì—¬ê¸°ë¶€í„° í•˜ê¸°   ìœ íŠ­ì—ì„œ ë„¤ì´ë²„ í˜•ì‹ì— ë§ê²Œ ê³ ì¹˜ê¸° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd9d66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186 entries, 0 to 185\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   í‚¤ì›Œë“œ             186 non-null    object \n",
      " 1   ì–¸ë¡ ì‚¬             186 non-null    object \n",
      " 2   ê²Œì‹œì¼             186 non-null    object \n",
      " 3   ë‰´ìŠ¤ì œëª©            186 non-null    object \n",
      " 4   url             186 non-null    object \n",
      " 5   gpt_sentiment   186 non-null    object \n",
      " 6   gpt_confidence  186 non-null    float64\n",
      " 7   videoId         186 non-null    object \n",
      " 8   comment_cnt     186 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 13.2+ KB\n"
     ]
    }
   ],
   "source": [
    "posts = pd.read_excel(os.path.join(BASE_PATH, f\"{'GS'}_posts_with_comment_cnt.xlsx\"))\n",
    "comments = pd.read_excel(os.path.join(BASE_PATH, f\"{'GS'}_ëŒ“ê¸€ê°ì •ë¶„ì„_GPT_labeled.xlsx\"))\n",
    "\n",
    "posts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e632f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593 entries, 0 to 592\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   channel         593 non-null    object \n",
      " 1   url             593 non-null    object \n",
      " 2   title           0 non-null      float64\n",
      " 3   post_date       593 non-null    object \n",
      " 4   comment         593 non-null    object \n",
      " 5   gpt_sentiment   593 non-null    object \n",
      " 6   gpt_confidence  593 non-null    float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 32.6+ KB\n"
     ]
    }
   ],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b6e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ GS ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸš€ KT ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸš€ ë¡¯ë° ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸš€ SKT ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 1. ê¸°ë³¸ ì„¤ì •\n",
    "# ============================================================\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 2. í•¨ìˆ˜ ì •ì˜\n",
    "# ============================================================\n",
    "\n",
    "def load_data(issue):\n",
    "    \"\"\"posts / comments ë°ì´í„° ë¡œë“œ + ë‚ ì§œ ì²˜ë¦¬\"\"\"\n",
    "    posts = pd.read_excel(os.path.join(BASE_PATH, f\"{issue}_posts_with_comment_cnt.xlsx\"))\n",
    "    comments = pd.read_excel(os.path.join(BASE_PATH, f\"{issue}_ëŒ“ê¸€ê°ì •ë¶„ì„_GPT_labeled.xlsx\"))\n",
    "    \n",
    "    posts[\"publishedAt\"] = pd.to_datetime(posts[\"ê²Œì‹œì¼\"], errors=\"coerce\", utc=True)\n",
    "    comments[\"publishedAt\"] = pd.to_datetime(comments[\"post_date\"], errors=\"coerce\", utc=True)\n",
    "    \n",
    "    return posts, comments\n",
    "\n",
    "\n",
    "def merge_post_comment(posts, comments):\n",
    "    \"\"\"ê²Œì‹œê¸€ê³¼ ëŒ“ê¸€ ë°ì´í„° ë³‘í•© + ì‹œê°„ì°¨ ê³„ì‚°\"\"\"\n",
    "    merged = pd.merge(\n",
    "        posts,\n",
    "        comments[[\"url\", \"publishedAt\", \"gpt_sentiment\"]],\n",
    "        on=\"url\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_post\", \"_comment\")\n",
    "    )\n",
    "\n",
    "    merged[\"hours_after\"] = (\n",
    "        (merged[\"publishedAt_comment\"] - merged[\"publishedAt_post\"]).dt.total_seconds() / 3600\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "\n",
    "def calc_alignment(merged):\n",
    "    \"\"\"ê²Œì‹œê¸€-ëŒ“ê¸€ ê°ì • ì •ë ¬ë„ ê³„ì‚°\"\"\"\n",
    "    def to_score(x):\n",
    "        if isinstance(x, str):\n",
    "            if \"ë¶€ì •\" in x:\n",
    "                return -1\n",
    "            elif \"ê¸ì •\" in x:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    merged[\"post_s\"] = merged[\"gpt_sentiment_post\"].apply(to_score)\n",
    "    merged[\"cmt_s\"] = merged[\"gpt_sentiment_comment\"].apply(to_score)\n",
    "\n",
    "    # videoId ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ url ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”\n",
    "    grouped = (\n",
    "        merged.groupby(\"url\")\n",
    "        .agg(\n",
    "            í‚¤ì›Œë“œ=(\"í‚¤ì›Œë“œ\", \"first\"),\n",
    "            ì–¸ë¡ ì‚¬=(\"ì–¸ë¡ ì‚¬\", \"first\"),\n",
    "            ë‰´ìŠ¤ì œëª©=(\"ë‰´ìŠ¤ì œëª©\", \"first\"),\n",
    "            ê²Œì‹œì¼=(\"ê²Œì‹œì¼\", \"first\"),\n",
    "            post_sentiment=(\"gpt_sentiment_post\", \"first\"),\n",
    "            post_s=(\"post_s\", \"first\"),\n",
    "            cmt_s_mean=(\"cmt_s\", \"mean\"),\n",
    "            comment_cnt=(\"comment_cnt\", \"first\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # ê°ì • ì°¨ì´\n",
    "    grouped[\"abs_sent_gap\"] = (grouped[\"post_s\"] - grouped[\"cmt_s_mean\"]).abs().round(2)\n",
    "\n",
    "    # 3/6/9ì‹œê°„ ë‚´ ë™ì¼ê°ì • ëŒ“ê¸€ ìˆ˜ ì¹´ìš´íŠ¸\n",
    "    for h in [3, 6, 9]:\n",
    "        cond = (merged[\"hours_after\"] <= h) & (merged[\"post_s\"] == merged[\"cmt_s\"])\n",
    "        aligned = merged[cond].groupby(\"url\").size()\n",
    "        grouped[f\"aligned_cnt_{h}h\"] = grouped[\"url\"].map(aligned).fillna(0).astype(int)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 3. ì‹¤í–‰ ë£¨í”„\n",
    "# ============================================================\n",
    "for issue in ISSUES:\n",
    "    print(f\"\\nğŸš€ {issue} ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "    posts, comments = load_data(issue)\n",
    "    merged = merge_post_comment(posts, comments)\n",
    "    result = calc_alignment(merged)\n",
    "\n",
    "    # Excel ì €ì¥ ì „ timezone ì œê±°\n",
    "    for col in merged.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "        merged[col] = merged[col].dt.tz_localize(None)\n",
    "    for col in result.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "        result[col] = result[col].dt.tz_localize(None)\n",
    "\n",
    "    # ì €ì¥\n",
    "    merged.to_excel(os.path.join(BASE_PATH, f\"{issue}_merged_timegap.xlsx\"), index=False, engine=\"openpyxl\")\n",
    "    result.to_excel(os.path.join(BASE_PATH, f\"{issue}_final_post_level.xlsx\"), index=False, engine=\"openpyxl\")\n",
    "\n",
    "    \n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ ì²˜ë¦¬ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddd7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ gs: scriptì— 'context' ì»¬ëŸ¼ ì—†ìŒ â€” ë¹ˆê°’ ì¶”ê°€\n",
      "âœ… GS_final_post_level.xlsx ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” context ì •í™•íˆ ë³‘í•©ë¨ (186í–‰)\n",
      "âš ï¸ kt: scriptì— 'context' ì»¬ëŸ¼ ì—†ìŒ â€” ë¹ˆê°’ ì¶”ê°€\n",
      "âœ… KT_final_post_level.xlsx ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” context ì •í™•íˆ ë³‘í•©ë¨ (436í–‰)\n",
      "âš ï¸ lotte: scriptì— 'context' ì»¬ëŸ¼ ì—†ìŒ â€” ë¹ˆê°’ ì¶”ê°€\n",
      "âœ… ë¡¯ë°_final_post_level.xlsx ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” context ì •í™•íˆ ë³‘í•©ë¨ (340í–‰)\n",
      "âš ï¸ skt: scriptì— 'context' ì»¬ëŸ¼ ì—†ìŒ â€” ë¹ˆê°’ ì¶”ê°€\n",
      "âœ… SKT_final_post_level.xlsx ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” context ì •í™•íˆ ë³‘í•©ë¨ (881í–‰)\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ íŒŒì¼ì— scriptì˜ context ì»¬ëŸ¼ì´ URL ê¸°ì¤€ìœ¼ë¡œ ì •ìƒ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê²½ë¡œ ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "base_path = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\"\n",
    "post_path = os.path.join(base_path, \"final_post_1\")\n",
    "script_path = os.path.join(base_path, \"script\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ì´ìŠˆë³„ ë§¤í•‘\n",
    "# ------------------------------------------------------------\n",
    "issue_map = {\n",
    "    \"gs\": \"GS_final_post_level.xlsx\",\n",
    "    \"kt\": \"KT_final_post_level.xlsx\",\n",
    "    \"lotte\": \"ë¡¯ë°_final_post_level.xlsx\",\n",
    "    \"skt\": \"SKT_final_post_level.xlsx\"\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ ë³‘í•© ì‹¤í–‰ (url ê¸°ì¤€ìœ¼ë¡œ scriptì˜ contextë§Œ ë¶™ì´ê¸°)\n",
    "# ------------------------------------------------------------\n",
    "for issue, post_filename in issue_map.items():\n",
    "    post_file = os.path.join(post_path, post_filename)\n",
    "    script_file = os.path.join(script_path, f\"{issue}_script.csv\")\n",
    "\n",
    "    if not os.path.exists(post_file):\n",
    "        print(f\"âš ï¸ {post_filename} ì—†ìŒ â€” ê±´ë„ˆëœ€\")\n",
    "        continue\n",
    "    if not os.path.exists(script_file):\n",
    "        print(f\"âš ï¸ {issue}_script.csv ì—†ìŒ â€” ê±´ë„ˆëœ€\")\n",
    "        continue\n",
    "\n",
    "    # ì—‘ì…€, CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    post_df = pd.read_excel(post_file)\n",
    "    script_df = pd.read_csv(script_file)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4ï¸âƒ£ URL ì „ì²˜ë¦¬ (ê³µë°±, ëŒ€ì†Œë¬¸ì, http/https ì •ê·œí™”)\n",
    "    # --------------------------------------------------------\n",
    "    for df in [post_df, script_df]:\n",
    "        df['url'] = df['url'].astype(str).str.strip().str.lower()\n",
    "        df['url'] = df['url'].str.replace('https://', 'http://', regex=False)\n",
    "        df['url'] = df['url'].str.rstrip('/')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5ï¸âƒ£ ë³‘í•© (contextë§Œ ì¶”ê°€)\n",
    "    # --------------------------------------------------------\n",
    "    if 'context' not in script_df.columns:\n",
    "        print(f\"âš ï¸ {issue}: scriptì— 'context' ì»¬ëŸ¼ ì—†ìŒ â€” ë¹ˆê°’ ì¶”ê°€\")\n",
    "        script_df['context'] = \"\"\n",
    "\n",
    "    # ê¸°ì¡´ post_dfì— context ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œê±° í›„ ê°±ì‹ \n",
    "    if 'context' in post_df.columns:\n",
    "        post_df = post_df.drop(columns=['context'])\n",
    "\n",
    "    # url ê¸°ì¤€ ë³‘í•©\n",
    "    merged = post_df.merge(\n",
    "        script_df[['url', 'context']],\n",
    "        on='url',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6ï¸âƒ£ ë®ì–´ì“°ê¸° ì €ì¥ (ê°™ì€ íŒŒì¼ëª…)\n",
    "    # --------------------------------------------------------\n",
    "    merged.to_excel(post_file, index=False, engine='openpyxl')\n",
    "    print(f\"âœ… {post_filename} ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” context ì •í™•íˆ ë³‘í•©ë¨ ({len(merged)}í–‰)\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ íŒŒì¼ì— scriptì˜ context ì»¬ëŸ¼ì´ URL ê¸°ì¤€ìœ¼ë¡œ ì •ìƒ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46f59bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "ğŸ“‚ GS ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸\n",
      "============================\n",
      "ğŸŸ¦ ìœ íŠœë¸Œ (GS): C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\\GS_final_post_level.xlsx\n",
      "â†’ ['videoId', 'channelTitle', 'publishedAt', 'viewCount', 'likeCount', 'commentCount', 'subscriberCount', 'text_raw', 'gpt_post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n",
      "ğŸŸ© ë„¤ì´ë²„ (GS): C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\\GS_final_post_level.xlsx\n",
      "â†’ ['url', 'í‚¤ì›Œë“œ', 'ì–¸ë¡ ì‚¬', 'ë‰´ìŠ¤ì œëª©', 'ê²Œì‹œì¼', 'post_sentiment', 'post_s', 'cmt_s_mean', 'comment_cnt', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n",
      "\n",
      "============================\n",
      "ğŸ“‚ KT ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸\n",
      "============================\n",
      "ğŸŸ¦ ìœ íŠœë¸Œ (KT): C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\\KT_final_post_level.xlsx\n",
      "â†’ ['videoId', 'channelTitle', 'publishedAt', 'viewCount', 'likeCount', 'commentCount', 'subscriberCount', 'text_raw', 'gpt_post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n",
      "ğŸŸ© ë„¤ì´ë²„ (KT): C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\\KT_final_post_level.xlsx\n",
      "â†’ ['url', 'í‚¤ì›Œë“œ', 'ì–¸ë¡ ì‚¬', 'ë‰´ìŠ¤ì œëª©', 'ê²Œì‹œì¼', 'post_sentiment', 'post_s', 'cmt_s_mean', 'comment_cnt', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n",
      "\n",
      "============================\n",
      "ğŸ“‚ ë¡¯ë° ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸\n",
      "============================\n",
      "ğŸŸ¦ ìœ íŠœë¸Œ (ë¡¯ë°): C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\\ë¡¯ë°_final_post_level.xlsx\n",
      "â†’ ['videoId', 'channelTitle', 'publishedAt', 'viewCount', 'likeCount', 'commentCount', 'subscriberCount', 'text_raw', 'gpt_post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n",
      "ğŸŸ© ë„¤ì´ë²„ (ë¡¯ë°): C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\\ë¡¯ë°_final_post_level.xlsx\n",
      "â†’ ['url', 'í‚¤ì›Œë“œ', 'ì–¸ë¡ ì‚¬', 'ë‰´ìŠ¤ì œëª©', 'ê²Œì‹œì¼', 'post_sentiment', 'post_s', 'cmt_s_mean', 'comment_cnt', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n",
      "\n",
      "============================\n",
      "ğŸ“‚ SKT ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸\n",
      "============================\n",
      "ğŸŸ¦ ìœ íŠœë¸Œ (SKT): C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\\SKT_final_post_level.xlsx\n",
      "â†’ ['videoId', 'channelTitle', 'publishedAt', 'viewCount', 'likeCount', 'commentCount', 'subscriberCount', 'text_raw', 'gpt_post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n",
      "ğŸŸ© ë„¤ì´ë²„ (SKT): C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\\SKT_final_post_level.xlsx\n",
      "â†’ ['url', 'í‚¤ì›Œë“œ', 'ì–¸ë¡ ì‚¬', 'ë‰´ìŠ¤ì œëª©', 'ê²Œì‹œì¼', 'post_sentiment', 'post_s', 'cmt_s_mean', 'comment_cnt', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 1. ê²½ë¡œ ì„¤ì •\n",
    "# ============================================================\n",
    "YOUTUBE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\"\n",
    "NAVER_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 2. ê° íŒŒì¼ ì»¬ëŸ¼ëª… ì¶œë ¥\n",
    "# ============================================================\n",
    "for issue in ISSUES:\n",
    "    yt_path = os.path.join(YOUTUBE_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "    nv_path = os.path.join(NAVER_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\"ğŸ“‚ {issue} ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸\")\n",
    "    print(\"============================\")\n",
    "\n",
    "    if os.path.exists(yt_path):\n",
    "        yt_cols = pd.read_excel(yt_path, nrows=1).columns.tolist()\n",
    "        print(f\"ğŸŸ¦ ìœ íŠœë¸Œ ({issue}): {yt_path}\")\n",
    "        print(f\"â†’ {yt_cols}\\n\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ ìœ íŠœë¸Œ íŒŒì¼ ì—†ìŒ: {yt_path}\")\n",
    "\n",
    "    if os.path.exists(nv_path):\n",
    "        nv_cols = pd.read_excel(nv_path, nrows=1).columns.tolist()\n",
    "        print(f\"ğŸŸ© ë„¤ì´ë²„ ({issue}): {nv_path}\")\n",
    "        print(f\"â†’ {nv_cols}\\n\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ ë„¤ì´ë²„ íŒŒì¼ ì—†ìŒ: {nv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1254288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS í†µí•© ì™„ë£Œ (208í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\GS_í†µí•©_final.xlsx\n",
      "âœ… KT í†µí•© ì™„ë£Œ (785í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\KT_í†µí•©_final.xlsx\n",
      "âœ… ë¡¯ë° í†µí•© ì™„ë£Œ (840í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\ë¡¯ë°_í†µí•©_final.xlsx\n",
      "âœ… SKT í†µí•© ì™„ë£Œ (1375í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\SKT_í†µí•©_final.xlsx\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (date ë§¨ ì™¼ìª½ + comment_cnt ê²°ì¸¡ì¹˜ 0 ì²˜ë¦¬ ì™„ë£Œ)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 1. ê¸°ë³¸ ì„¤ì •\n",
    "# ============================================================\n",
    "YOUTUBE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\"\n",
    "NAVER_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\"\n",
    "\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 2. ë‚ ì§œ í†µì¼ í•¨ìˆ˜\n",
    "# ============================================================\n",
    "\n",
    "def normalize_date_column(df):\n",
    "    \"\"\"YouTube: publishedAt â†’ date / Naver: ê²Œì‹œì¼ â†’ date\"\"\"\n",
    "    if \"publishedAt\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"publishedAt\"], errors=\"coerce\").dt.date\n",
    "    elif \"ê²Œì‹œì¼\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"ê²Œì‹œì¼\"], errors=\"coerce\").dt.date\n",
    "    else:\n",
    "        df[\"date\"] = pd.NaT\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 3. í”Œë«í¼ë³„ ë¡œë“œ í•¨ìˆ˜\n",
    "# ============================================================\n",
    "\n",
    "def load_youtube(issue):\n",
    "    path = os.path.join(YOUTUBE_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ ìœ íŠœë¸Œ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "    df = normalize_date_column(df)\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"videoId\": \"content_id\",\n",
    "            \"gpt_post_sentiment\": \"post_sentiment\",\n",
    "            \"text_raw\": \"title_or_text\"\n",
    "        }\n",
    "    )\n",
    "    df[\"channel\"] = \"ìœ íŠœë¸Œ\"\n",
    "\n",
    "    keep_cols = [\n",
    "        \"content_id\", \"channel\", \"channelTitle\", \"title_or_text\",\n",
    "        \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "        \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "        \"viewCount\", \"likeCount\", \"commentCount\", \"subscriberCount\", \"date\"\n",
    "    ]\n",
    "    df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "    # ìœ íŠœë¸Œì—” comment_cnt ì—†ìœ¼ë¯€ë¡œ commentCount â†’ comment_cntë¡œ í†µí•©\n",
    "    if \"commentCount\" in df.columns:\n",
    "        df = df.rename(columns={\"commentCount\": \"comment_cnt\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_naver(issue):\n",
    "    path = os.path.join(NAVER_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ ë„¤ì´ë²„ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "    df = normalize_date_column(df)\n",
    "    df = df.drop(columns=[\"í‚¤ì›Œë“œ\"], errors=\"ignore\")\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"url\": \"content_id\",\n",
    "            \"ë‰´ìŠ¤ì œëª©\": \"title_or_text\"\n",
    "        }\n",
    "    )\n",
    "    df[\"channel\"] = \"ë„¤ì´ë²„\"\n",
    "\n",
    "    keep_cols = [\n",
    "        \"content_id\", \"channel\", \"ì–¸ë¡ ì‚¬\", \"title_or_text\",\n",
    "        \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "        \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "        \"comment_cnt\", \"date\",\"content\"\n",
    "    ]\n",
    "    df = df[[c for c in keep_cols if c in df.columns]]\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 4. ë³‘í•© ë° ì €ì¥\n",
    "# ============================================================\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "for issue in ISSUES:\n",
    "    yt_df = load_youtube(issue)\n",
    "    nv_df = load_naver(issue)\n",
    "\n",
    "    combined = pd.concat([yt_df, nv_df], ignore_index=True)\n",
    "\n",
    "    # âœ… comment_cnt NaN â†’ 0\n",
    "    if \"comment_cnt\" in combined.columns:\n",
    "        combined[\"comment_cnt\"] = combined[\"comment_cnt\"].fillna(0).astype(int)\n",
    "\n",
    "    # âœ… ë‚ ì§œ ê¸°ì¤€ ì •ë ¬\n",
    "    combined = combined.sort_values(by=\"date\")\n",
    "\n",
    "    # âœ… date ì»¬ëŸ¼ì„ ê°€ì¥ ì™¼ìª½ìœ¼ë¡œ ì´ë™\n",
    "    cols = [\"date\"] + [c for c in combined.columns if c != \"date\"]\n",
    "    combined = combined[cols]\n",
    "\n",
    "    # ì €ì¥\n",
    "    output_file = os.path.join(OUTPUT_PATH, f\"{issue}_í†µí•©_final.xlsx\")\n",
    "    combined.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} í†µí•© ì™„ë£Œ ({len(combined)}í–‰ ì €ì¥) â†’ {output_file}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (date ë§¨ ì™¼ìª½ + comment_cnt ê²°ì¸¡ì¹˜ 0 ì²˜ë¦¬ ì™„ë£Œ)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b5dc0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS í†µí•© ì™„ë£Œ (208í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\GS_í†µí•©_final.xlsx\n",
      "âœ… KT í†µí•© ì™„ë£Œ (785í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\KT_í†µí•©_final.xlsx\n",
      "âœ… ë¡¯ë° í†µí•© ì™„ë£Œ (840í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\ë¡¯ë°_í†µí•©_final.xlsx\n",
      "âœ… SKT í†µí•© ì™„ë£Œ (1375í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\\SKT_í†µí•©_final.xlsx\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (date ë§¨ ì™¼ìª½ + comment_cnt ê²°ì¸¡ì¹˜ 0 ì²˜ë¦¬ ì™„ë£Œ)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 1. ê¸°ë³¸ ì„¤ì •\n",
    "# # ============================================================\n",
    "# YOUTUBE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\"\n",
    "# NAVER_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\"\n",
    "# OUTPUT_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œë³„\"\n",
    "\n",
    "# ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 2. ë‚ ì§œ í†µì¼ í•¨ìˆ˜\n",
    "# # ============================================================\n",
    "\n",
    "# def normalize_date_column(df):\n",
    "#     \"\"\"YouTube: publishedAt â†’ date / Naver: ê²Œì‹œì¼ â†’ date\"\"\"\n",
    "#     if \"publishedAt\" in df.columns:\n",
    "#         df[\"date\"] = pd.to_datetime(df[\"publishedAt\"], errors=\"coerce\").dt.date\n",
    "#     elif \"ê²Œì‹œì¼\" in df.columns:\n",
    "#         df[\"date\"] = pd.to_datetime(df[\"ê²Œì‹œì¼\"], errors=\"coerce\").dt.date\n",
    "#     else:\n",
    "#         df[\"date\"] = pd.NaT\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 3. í”Œë«í¼ë³„ ë¡œë“œ í•¨ìˆ˜\n",
    "# # ============================================================\n",
    "\n",
    "# def load_youtube(issue):\n",
    "#     path = os.path.join(YOUTUBE_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "#     if not os.path.exists(path):\n",
    "#         print(f\"âš ï¸ ìœ íŠœë¸Œ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     df = pd.read_excel(path)\n",
    "#     df = normalize_date_column(df)\n",
    "\n",
    "#     df = df.rename(\n",
    "#         columns={\n",
    "#             \"videoId\": \"content_id\",\n",
    "#             \"gpt_post_sentiment\": \"post_sentiment\",\n",
    "#             \"text_raw\": \"title_or_text\"\n",
    "#         }\n",
    "#     )\n",
    "#     df[\"channel\"] = \"ìœ íŠœë¸Œ\"\n",
    "\n",
    "#     keep_cols = [\n",
    "#         \"content_id\", \"channel\", \"channelTitle\", \"title_or_text\",\n",
    "#         \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "#         \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "#         \"viewCount\", \"likeCount\", \"commentCount\", \"subscriberCount\", \"date\"\n",
    "#     ]\n",
    "#     df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "#     # ìœ íŠœë¸Œì—” comment_cnt ì—†ìœ¼ë¯€ë¡œ commentCount â†’ comment_cntë¡œ í†µí•©\n",
    "#     if \"commentCount\" in df.columns:\n",
    "#         df = df.rename(columns={\"commentCount\": \"comment_cnt\"})\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def load_naver(issue):\n",
    "#     path = os.path.join(NAVER_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "#     if not os.path.exists(path):\n",
    "#         print(f\"âš ï¸ ë„¤ì´ë²„ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     df = pd.read_excel(path)\n",
    "#     df = normalize_date_column(df)\n",
    "#     df = df.drop(columns=[\"í‚¤ì›Œë“œ\"], errors=\"ignore\")\n",
    "\n",
    "#     df = df.rename(\n",
    "#         columns={\n",
    "#             \"url\": \"content_id\",\n",
    "#             \"ë‰´ìŠ¤ì œëª©\": \"title_or_text\"\n",
    "#         }\n",
    "#     )\n",
    "#     df[\"channel\"] = \"ë„¤ì´ë²„\"\n",
    "\n",
    "#     keep_cols = [\n",
    "#         \"content_id\", \"channel\", \"ì–¸ë¡ ì‚¬\", \"title_or_text\",\n",
    "#         \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "#         \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "#         \"comment_cnt\", \"date\",\"content\"\n",
    "#     ]\n",
    "#     df = df[[c for c in keep_cols if c in df.columns]]\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 4. ë³‘í•© ë° ì €ì¥\n",
    "# # ============================================================\n",
    "\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# for issue in ISSUES:\n",
    "#     yt_df = load_youtube(issue)\n",
    "#     nv_df = load_naver(issue)\n",
    "\n",
    "#     combined = pd.concat([yt_df, nv_df], ignore_index=True)\n",
    "\n",
    "#     # âœ… comment_cnt NaN â†’ 0\n",
    "#     if \"comment_cnt\" in combined.columns:\n",
    "#         combined[\"comment_cnt\"] = combined[\"comment_cnt\"].fillna(0).astype(int)\n",
    "\n",
    "#     # âœ… ë‚ ì§œ ê¸°ì¤€ ì •ë ¬\n",
    "#     combined = combined.sort_values(by=\"date\")\n",
    "\n",
    "#     # âœ… date ì»¬ëŸ¼ì„ ê°€ì¥ ì™¼ìª½ìœ¼ë¡œ ì´ë™\n",
    "#     cols = [\"date\"] + [c for c in combined.columns if c != \"date\"]\n",
    "#     combined = combined[cols]\n",
    "\n",
    "#     # ì €ì¥\n",
    "#     output_file = os.path.join(OUTPUT_PATH, f\"{issue}_í†µí•©_final.xlsx\")\n",
    "#     combined.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "#     print(f\"âœ… {issue} í†µí•© ì™„ë£Œ ({len(combined)}í–‰ ì €ì¥) â†’ {output_file}\")\n",
    "\n",
    "# print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (date ë§¨ ì™¼ìª½ + comment_cnt ê²°ì¸¡ì¹˜ 0 ì²˜ë¦¬ ì™„ë£Œ)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b20a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS ì™„ë£Œ (208í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\GS_í†µí•©_final.xlsx\n",
      "âœ… KT ì™„ë£Œ (785í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\KT_í†µí•©_final.xlsx\n",
      "âœ… ë¡¯ë° ì™„ë£Œ (840í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\ë¡¯ë°_í†µí•©_final.xlsx\n",
      "âœ… SKT ì™„ë£Œ (1375í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\SKT_í†µí•©_final.xlsx\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (ì‹œê°„ ì •ë³´ ìœ ì§€ + datetime ë§¨ ì™¼ìª½ + comment_cnt=0 ì²˜ë¦¬)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 1. ê¸°ë³¸ ì„¤ì •\n",
    "# ============================================================\n",
    "YOUTUBE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\"\n",
    "NAVER_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\"\n",
    "\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 2. ë‚ ì§œ + ì‹œê°„ í†µì¼ í•¨ìˆ˜ (KST ìœ ì§€)\n",
    "# ============================================================\n",
    "\n",
    "def normalize_datetime_column(df):\n",
    "    \"\"\"YouTube: publishedAt / Naver: ê²Œì‹œì¼ â†’ datetime (KST) ë³€í™˜, ì‹œê°„ í¬í•¨\"\"\"\n",
    "    if \"publishedAt\" in df.columns:\n",
    "        df[\"datetime\"] = (\n",
    "            pd.to_datetime(df[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "            .dt.tz_convert(\"Asia/Seoul\")\n",
    "            .dt.tz_localize(None)\n",
    "        )\n",
    "    elif \"ê²Œì‹œì¼\" in df.columns:\n",
    "        df[\"datetime\"] = (\n",
    "            pd.to_datetime(df[\"ê²Œì‹œì¼\"], errors=\"coerce\", utc=True)\n",
    "            .dt.tz_convert(\"Asia/Seoul\")\n",
    "            .dt.tz_localize(None)\n",
    "        )\n",
    "    else:\n",
    "        df[\"datetime\"] = pd.NaT\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 3. í”Œë«í¼ë³„ ë¡œë“œ í•¨ìˆ˜\n",
    "# ============================================================\n",
    "\n",
    "def load_youtube(issue):\n",
    "    path = os.path.join(YOUTUBE_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ ìœ íŠœë¸Œ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "    df = normalize_datetime_column(df)\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"videoId\": \"content_id\",\n",
    "            \"gpt_post_sentiment\": \"post_sentiment\",\n",
    "            \"text_raw\": \"title_or_text\"\n",
    "        }\n",
    "    )\n",
    "    df[\"channel\"] = \"ìœ íŠœë¸Œ\"\n",
    "\n",
    "    keep_cols = [\n",
    "        \"content_id\", \"channel\", \"channelTitle\", \"title_or_text\",\n",
    "        \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "        \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "        \"viewCount\", \"likeCount\", \"commentCount\", \"subscriberCount\", \"datetime\"\n",
    "    ]\n",
    "    df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "    # ìœ íŠœë¸Œ commentCount â†’ comment_cnt í†µì¼\n",
    "    if \"commentCount\" in df.columns:\n",
    "        df = df.rename(columns={\"commentCount\": \"comment_cnt\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_naver(issue):\n",
    "    path = os.path.join(NAVER_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ ë„¤ì´ë²„ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "    df = normalize_datetime_column(df)\n",
    "    df = df.drop(columns=[\"í‚¤ì›Œë“œ\"], errors=\"ignore\")\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"url\": \"content_id\",\n",
    "            \"ë‰´ìŠ¤ì œëª©\": \"title_or_text\"\n",
    "        }\n",
    "    )\n",
    "    df[\"channel\"] = \"ë„¤ì´ë²„\"\n",
    "\n",
    "    keep_cols = [\n",
    "        \"content_id\", \"channel\", \"ì–¸ë¡ ì‚¬\", \"title_or_text\",\n",
    "        \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "        \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "        \"comment_cnt\", \"datetime\",\"content\"\n",
    "    ]\n",
    "    df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 4. ë³‘í•© ë° ì €ì¥\n",
    "# ============================================================\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "for issue in ISSUES:\n",
    "    yt_df = load_youtube(issue)\n",
    "    nv_df = load_naver(issue)\n",
    "\n",
    "    combined = pd.concat([yt_df, nv_df], ignore_index=True)\n",
    "\n",
    "    # âœ… comment_cnt NaN â†’ 0\n",
    "    if \"comment_cnt\" in combined.columns:\n",
    "        combined[\"comment_cnt\"] = pd.to_numeric(combined[\"comment_cnt\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # âœ… ë‚ ì§œ+ì‹œê°„ ê¸°ì¤€ ì •ë ¬\n",
    "    combined = combined.sort_values(by=\"datetime\")\n",
    "\n",
    "    # âœ… datetime ì»¬ëŸ¼ì„ ê°€ì¥ ì™¼ìª½ìœ¼ë¡œ ì´ë™\n",
    "    cols = [\"datetime\"] + [c for c in combined.columns if c != \"datetime\"]\n",
    "    combined = combined[cols]\n",
    "\n",
    "    # ì €ì¥\n",
    "    output_file = os.path.join(OUTPUT_PATH, f\"{issue}_í†µí•©_final.xlsx\")\n",
    "    combined.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} ì™„ë£Œ ({len(combined)}í–‰ ì €ì¥) â†’ {output_file}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (ì‹œê°„ ì •ë³´ ìœ ì§€ + datetime ë§¨ ì™¼ìª½ + comment_cnt=0 ì²˜ë¦¬)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02943e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS ì™„ë£Œ (208í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\GS_í†µí•©_final.xlsx\n",
      "âœ… KT ì™„ë£Œ (785í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\KT_í†µí•©_final.xlsx\n",
      "âœ… ë¡¯ë° ì™„ë£Œ (840í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\ë¡¯ë°_í†µí•©_final.xlsx\n",
      "âœ… SKT ì™„ë£Œ (1375í–‰ ì €ì¥) â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\SKT_í†µí•©_final.xlsx\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (ì‹œê°„ ì •ë³´ ìœ ì§€ + datetime ë§¨ ì™¼ìª½ + comment_cnt=0 ì²˜ë¦¬)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 1. ê¸°ë³¸ ì„¤ì •\n",
    "# # ============================================================\n",
    "# YOUTUBE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\"\n",
    "# NAVER_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\final_post_1\"\n",
    "# OUTPUT_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\"\n",
    "\n",
    "# ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 2. ë‚ ì§œ + ì‹œê°„ í†µì¼ í•¨ìˆ˜ (KST ìœ ì§€)\n",
    "# # ============================================================\n",
    "\n",
    "# def normalize_datetime_column(df):\n",
    "#     \"\"\"YouTube: publishedAt / Naver: ê²Œì‹œì¼ â†’ datetime (KST) ë³€í™˜, ì‹œê°„ í¬í•¨\"\"\"\n",
    "#     if \"publishedAt\" in df.columns:\n",
    "#         df[\"datetime\"] = (\n",
    "#             pd.to_datetime(df[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "#             .dt.tz_convert(\"Asia/Seoul\")\n",
    "#             .dt.tz_localize(None)\n",
    "#         )\n",
    "#     elif \"ê²Œì‹œì¼\" in df.columns:\n",
    "#         df[\"datetime\"] = (\n",
    "#             pd.to_datetime(df[\"ê²Œì‹œì¼\"], errors=\"coerce\", utc=True)\n",
    "#             .dt.tz_convert(\"Asia/Seoul\")\n",
    "#             .dt.tz_localize(None)\n",
    "#         )\n",
    "#     else:\n",
    "#         df[\"datetime\"] = pd.NaT\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 3. í”Œë«í¼ë³„ ë¡œë“œ í•¨ìˆ˜\n",
    "# # ============================================================\n",
    "\n",
    "# def load_youtube(issue):\n",
    "#     path = os.path.join(YOUTUBE_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "#     if not os.path.exists(path):\n",
    "#         print(f\"âš ï¸ ìœ íŠœë¸Œ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     df = pd.read_excel(path)\n",
    "#     df = normalize_datetime_column(df)\n",
    "\n",
    "#     df = df.rename(\n",
    "#         columns={\n",
    "#             \"videoId\": \"content_id\",\n",
    "#             \"gpt_post_sentiment\": \"post_sentiment\",\n",
    "#             \"text_raw\": \"title_or_text\"\n",
    "#         }\n",
    "#     )\n",
    "#     df[\"channel\"] = \"ìœ íŠœë¸Œ\"\n",
    "\n",
    "#     keep_cols = [\n",
    "#         \"content_id\", \"channel\", \"channelTitle\", \"title_or_text\",\n",
    "#         \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "#         \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "#         \"viewCount\", \"likeCount\", \"commentCount\", \"subscriberCount\", \"datetime\"\n",
    "#     ]\n",
    "#     df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "#     # ìœ íŠœë¸Œ commentCount â†’ comment_cnt í†µì¼\n",
    "#     if \"commentCount\" in df.columns:\n",
    "#         df = df.rename(columns={\"commentCount\": \"comment_cnt\"})\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def load_naver(issue):\n",
    "#     path = os.path.join(NAVER_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "#     if not os.path.exists(path):\n",
    "#         print(f\"âš ï¸ ë„¤ì´ë²„ íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     df = pd.read_excel(path)\n",
    "#     df = normalize_datetime_column(df)\n",
    "#     df = df.drop(columns=[\"í‚¤ì›Œë“œ\"], errors=\"ignore\")\n",
    "\n",
    "#     df = df.rename(\n",
    "#         columns={\n",
    "#             \"url\": \"content_id\",\n",
    "#             \"ë‰´ìŠ¤ì œëª©\": \"title_or_text\"\n",
    "#         }\n",
    "#     )\n",
    "#     df[\"channel\"] = \"ë„¤ì´ë²„\"\n",
    "\n",
    "#     keep_cols = [\n",
    "#         \"content_id\", \"channel\", \"ì–¸ë¡ ì‚¬\", \"title_or_text\",\n",
    "#         \"post_sentiment\", \"post_s\", \"cmt_s_mean\", \"abs_sent_gap\",\n",
    "#         \"aligned_cnt_3h\", \"aligned_cnt_6h\", \"aligned_cnt_9h\",\n",
    "#         \"comment_cnt\", \"datetime\"\n",
    "#     ]\n",
    "#     df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # âœ… 4. ë³‘í•© ë° ì €ì¥\n",
    "# # ============================================================\n",
    "\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# for issue in ISSUES:\n",
    "#     yt_df = load_youtube(issue)\n",
    "#     nv_df = load_naver(issue)\n",
    "\n",
    "#     combined = pd.concat([yt_df, nv_df], ignore_index=True)\n",
    "\n",
    "#     # âœ… comment_cnt NaN â†’ 0\n",
    "#     if \"comment_cnt\" in combined.columns:\n",
    "#         combined[\"comment_cnt\"] = pd.to_numeric(combined[\"comment_cnt\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "#     # âœ… ë‚ ì§œ+ì‹œê°„ ê¸°ì¤€ ì •ë ¬\n",
    "#     combined = combined.sort_values(by=\"datetime\")\n",
    "\n",
    "#     # âœ… datetime ì»¬ëŸ¼ì„ ê°€ì¥ ì™¼ìª½ìœ¼ë¡œ ì´ë™\n",
    "#     cols = [\"datetime\"] + [c for c in combined.columns if c != \"datetime\"]\n",
    "#     combined = combined[cols]\n",
    "\n",
    "#     # ì €ì¥\n",
    "#     output_file = os.path.join(OUTPUT_PATH, f\"{issue}_í†µí•©_final.xlsx\")\n",
    "#     combined.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "#     print(f\"âœ… {issue} ì™„ë£Œ ({len(combined)}í–‰ ì €ì¥) â†’ {output_file}\")\n",
    "\n",
    "# print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ì™„ë£Œ (ì‹œê°„ ì •ë³´ ìœ ì§€ + datetime ë§¨ ì™¼ìª½ + comment_cnt=0 ì²˜ë¦¬)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a341852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>content_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title_or_text</th>\n",
       "      <th>post_sentiment</th>\n",
       "      <th>post_s</th>\n",
       "      <th>cmt_s_mean</th>\n",
       "      <th>abs_sent_gap</th>\n",
       "      <th>aligned_cnt_3h</th>\n",
       "      <th>aligned_cnt_6h</th>\n",
       "      <th>aligned_cnt_9h</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>comment_cnt</th>\n",
       "      <th>subscriberCount</th>\n",
       "      <th>ì–¸ë¡ ì‚¬</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-22 15:29:35</td>\n",
       "      <td>f1N0C4HixGE</td>\n",
       "      <td>ìœ íŠœë¸Œ</td>\n",
       "      <td>TS ë©¤ë²„ì‹­ í´ëŸ½</td>\n",
       "      <td>SKí…”ë ˆì½¤ ìœ ì‹¬ ì •ë³´ ìœ ì¶œ? ì˜ˆë°©ë²•ì€?  #skí…”ë ˆì½¤ #ìœ ì‹¬ì •ë³´ìœ ì¶œ  #skt ì•ˆë…•...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2085.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-22 16:56:41</td>\n",
       "      <td>F6Qi4wMfzxI</td>\n",
       "      <td>ìœ íŠœë¸Œ</td>\n",
       "      <td>ì´ë°ì¼ë¦¬TV</td>\n",
       "      <td>í†µì‹ ì‚¬ 1ìœ„ â€˜SKí…”ë ˆì½¤â€™, ì™¸ë¶€ í•´í‚¹ìœ¼ë¡œ ê³ ê° ìœ ì‹¬ ì •ë³´ ìœ ì¶œ (20250422)...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1751.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>343000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-22 17:36:27</td>\n",
       "      <td>aFr3emetb9M</td>\n",
       "      <td>ìœ íŠœë¸Œ</td>\n",
       "      <td>cocojuanì½”ì½”ì¥¬ì•ˆ</td>\n",
       "      <td>SKí…”ë ˆì½¤ í•´í‚¹ ì‚¬ê³  ìœ ì‹¬ ì •ë³´ ìœ ì¶œ, ìœ ì‹¬ë³´í˜¸ì„œë¹„ìŠ¤ ê°€ì… ë°©ë²• SKT ìœ ì‹¬ ì •ë³´ ...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7800.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-22 19:12:14</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/028/000...</td>\n",
       "      <td>ë„¤ì´ë²„</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SKT â€œí•´í‚¹ ë‹¹í–ˆë‹¤â€¦ê°€ì…ì ìœ ì‹¬ ì •ë³´ ìœ ì¶œ ì˜ì‹¬â€ ìì§„ì‹ ê³ </td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.538462</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>í•œê²¨ë ˆ</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/629/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-22 23:34:30</td>\n",
       "      <td>V71eooP07q4</td>\n",
       "      <td>ìœ íŠœë¸Œ</td>\n",
       "      <td>Cha Cha</td>\n",
       "      <td>SKí…”ë ˆì½¤ì„œí•´í‚¹ í”¼í•´ë°œìƒ ê°€ì…ì ìœ ì‹¬ì •ë³´ ìœ ì¶œ ì•…ì„±ì½”ë“œ ê³µê²©ë°›ì•„#SKT#í•´í‚¹#ì² ì €íˆ...</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1361.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>62600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                                         content_id  \\\n",
       "0 2025-04-22 15:29:35                                        f1N0C4HixGE   \n",
       "1 2025-04-22 16:56:41                                        F6Qi4wMfzxI   \n",
       "2 2025-04-22 17:36:27                                        aFr3emetb9M   \n",
       "3 2025-04-22 19:12:14  https://n.news.naver.com/mnews/article/028/000...   \n",
       "4 2025-04-22 23:34:30                                        V71eooP07q4   \n",
       "\n",
       "  channel  channelTitle                                      title_or_text  \\\n",
       "0     ìœ íŠœë¸Œ     TS ë©¤ë²„ì‹­ í´ëŸ½  SKí…”ë ˆì½¤ ìœ ì‹¬ ì •ë³´ ìœ ì¶œ? ì˜ˆë°©ë²•ì€?  #skí…”ë ˆì½¤ #ìœ ì‹¬ì •ë³´ìœ ì¶œ  #skt ì•ˆë…•...   \n",
       "1     ìœ íŠœë¸Œ        ì´ë°ì¼ë¦¬TV  í†µì‹ ì‚¬ 1ìœ„ â€˜SKí…”ë ˆì½¤â€™, ì™¸ë¶€ í•´í‚¹ìœ¼ë¡œ ê³ ê° ìœ ì‹¬ ì •ë³´ ìœ ì¶œ (20250422)...   \n",
       "2     ìœ íŠœë¸Œ  cocojuanì½”ì½”ì¥¬ì•ˆ  SKí…”ë ˆì½¤ í•´í‚¹ ì‚¬ê³  ìœ ì‹¬ ì •ë³´ ìœ ì¶œ, ìœ ì‹¬ë³´í˜¸ì„œë¹„ìŠ¤ ê°€ì… ë°©ë²• SKT ìœ ì‹¬ ì •ë³´ ...   \n",
       "3     ë„¤ì´ë²„           NaN                  SKT â€œí•´í‚¹ ë‹¹í–ˆë‹¤â€¦ê°€ì…ì ìœ ì‹¬ ì •ë³´ ìœ ì¶œ ì˜ì‹¬â€ ìì§„ì‹ ê³    \n",
       "4     ìœ íŠœë¸Œ       Cha Cha  SKí…”ë ˆì½¤ì„œí•´í‚¹ í”¼í•´ë°œìƒ ê°€ì…ì ìœ ì‹¬ì •ë³´ ìœ ì¶œ ì•…ì„±ì½”ë“œ ê³µê²©ë°›ì•„#SKT#í•´í‚¹#ì² ì €íˆ...   \n",
       "\n",
       "  post_sentiment  post_s  cmt_s_mean  abs_sent_gap  aligned_cnt_3h  \\\n",
       "0             ë¶€ì •      -1    1.000000          2.00               0   \n",
       "1             ë¶€ì •      -1    0.000000          1.00               0   \n",
       "2             ë¶€ì •      -1    0.000000          1.00               0   \n",
       "3             ë¶€ì •      -1   -0.538462          0.46               2   \n",
       "4             ë¶€ì •      -1    0.000000          1.00               0   \n",
       "\n",
       "   aligned_cnt_6h  aligned_cnt_9h  viewCount  likeCount  comment_cnt  \\\n",
       "0               0               0     2085.0       13.0            1   \n",
       "1               0               0     1751.0       15.0            0   \n",
       "2               0               0     7800.0       71.0            5   \n",
       "3               3               3        NaN        NaN           13   \n",
       "4               0               0     1361.0        8.0            0   \n",
       "\n",
       "   subscriberCount  ì–¸ë¡ ì‚¬                                            content  \n",
       "0           1970.0  NaN                                                NaN  \n",
       "1         343000.0  NaN                                                NaN  \n",
       "2           5600.0  NaN                                                NaN  \n",
       "3              NaN  í•œê²¨ë ˆ  https://n.news.naver.com/mnews/article/629/000...  \n",
       "4          62600.0  NaN                                                NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\\SKT_í†µí•©_final.xlsx\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "053ff88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'content_id', 'channel', 'channelTitle', 'title_or_text',\n",
       "       'post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap',\n",
       "       'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h', 'viewCount',\n",
       "       'likeCount', 'comment_cnt', 'subscriberCount', 'ì–¸ë¡ ì‚¬'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2610673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ ì´ìŠˆë³„ ìœ íŠœë¸Œ ì˜ìƒÂ·ëŒ“ê¸€ ìš”ì•½í‘œ\n",
      "\n",
      "| ì´ìŠˆ   |   ì±„ë„ ìˆ˜ |   ìœ íŠœë¸Œ ì˜ìƒ ìˆ˜ |   ìœ íŠœë¸Œ ì´ ëŒ“ê¸€ ìˆ˜ |   í‰ê·  ëŒ“ê¸€/ì˜ìƒ |\n",
      "|:-------|----------:|-----------------:|--------------------:|-----------------:|\n",
      "| SKT    |       354 |              494 |               68249 |           138.16 |\n",
      "| KT     |       204 |              349 |               28770 |            82.44 |\n",
      "| ë¡¯ë°   |       243 |              500 |               16194 |            32.39 |\n",
      "| GS     |        17 |               22 |                 215 |             9.77 |\n",
      "\n",
      "âœ… ì €ì¥ ì™„ë£Œ: C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\\ì´ìŠˆë³„_ìœ íŠœë¸Œ_ì „ìš©_ìš”ì•½.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê²½ë¡œ ë° ì´ìŠˆ ëª©ë¡\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\YouTube_ê²°ê³¼\\final_post\"  # âœ… ìœ íŠœë¸Œ ì „ìš© ê²½ë¡œ\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "summary = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ì´ìŠˆë³„ ìœ íŠœë¸Œ ì˜ìƒÂ·ëŒ“ê¸€ ì§‘ê³„\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    file_path = os.path.join(BASE_PATH, f\"{issue}_final_post_level.xlsx\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ {issue} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # ì±„ë„ ì»¬ëŸ¼ ìë™ íƒìƒ‰\n",
    "    channel_col = None\n",
    "    for col in df.columns:\n",
    "        if \"channel\" in col.lower():\n",
    "            channel_col = col\n",
    "            break\n",
    "\n",
    "    # -------------------------------\n",
    "    # ğŸ¯ ì§‘ê³„ ê³„ì‚°\n",
    "    # -------------------------------\n",
    "    num_channels = df[channel_col].nunique() if channel_col else None\n",
    "    num_videos = df[\"videoId\"].nunique() if \"videoId\" in df.columns else len(df)\n",
    "    total_comments = df[\"commentCount\"].sum() if \"commentCount\" in df.columns else 0\n",
    "    avg_comments = round(total_comments / num_videos, 2) if num_videos > 0 else 0\n",
    "\n",
    "    summary.append({\n",
    "        \"ì´ìŠˆ\": issue,\n",
    "        \"ì±„ë„ ìˆ˜\": num_channels,\n",
    "        \"ìœ íŠœë¸Œ ì˜ìƒ ìˆ˜\": num_videos,\n",
    "        \"ìœ íŠœë¸Œ ì´ ëŒ“ê¸€ ìˆ˜\": total_comments,\n",
    "        \"í‰ê·  ëŒ“ê¸€/ì˜ìƒ\": avg_comments\n",
    "    })\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ ê²°ê³¼ í‘œ ìƒì„±\n",
    "# ------------------------------------------------------------\n",
    "df_summary = pd.DataFrame(summary).sort_values(\"ìœ íŠœë¸Œ ì´ ëŒ“ê¸€ ìˆ˜\", ascending=False)\n",
    "print(\"\\nğŸ¯ ì´ìŠˆë³„ ìœ íŠœë¸Œ ì˜ìƒÂ·ëŒ“ê¸€ ìš”ì•½í‘œ\\n\")\n",
    "print(df_summary.to_markdown(index=False))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4ï¸âƒ£ ì €ì¥\n",
    "# ------------------------------------------------------------\n",
    "output_path = os.path.join(BASE_PATH, \"ì´ìŠˆë³„_ìœ íŠœë¸Œ_ì „ìš©_ìš”ì•½.xlsx\")\n",
    "df_summary.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e2d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS ì™„ë£Œ â€” ê²Œì‹œê¸€ 186ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\GS_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ GS ì´ìŠˆ ëŒ“ê¸€ ì´ 593ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ… KT ì™„ë£Œ â€” ê²Œì‹œê¸€ 436ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\KT_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ KT ì´ìŠˆ ëŒ“ê¸€ ì´ 2,248ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ… ë¡¯ë° ì™„ë£Œ â€” ê²Œì‹œê¸€ 340ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\ë¡¯ë°_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ ë¡¯ë° ì´ìŠˆ ëŒ“ê¸€ ì´ 2,287ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ… SKT ì™„ë£Œ â€” ê²Œì‹œê¸€ 881ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\\SKT_posts_with_comment_cnt.xlsx\n",
      "ğŸ’¬ SKT ì´ìŠˆ ëŒ“ê¸€ ì´ 15,388ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ ê²Œì‹œê¸€ì— ëŒ“ê¸€ìˆ˜ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê²½ë¡œ ë° ì´ìŠˆ ëª©ë¡\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ëŒ“ê¸€ ìˆ˜ë¥¼ ê²Œì‹œê¸€ì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ + ì „ì²´ ëŒ“ê¸€ ê°œìˆ˜ ì¶œë ¥\n",
    "# ------------------------------------------------------------\n",
    "def add_comment_count_to_posts(issue):\n",
    "    \"\"\"ëŒ“ê¸€ ìˆ˜ë¥¼ ê²Œì‹œê¸€ íŒŒì¼ì— ì¶”ê°€í•œ ë’¤ ì €ì¥í•˜ë©°, ì „ì²´ ëŒ“ê¸€ ìˆ˜ë„ ì¶œë ¥\"\"\"\n",
    "    # íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    post_path = os.path.join(BASE_PATH, f\"{issue}_ë‰´ìŠ¤ê°ì •ë¶„ì„_GPT_labeled.xlsx\")\n",
    "    cmt_path = os.path.join(BASE_PATH, f\"{issue}_ëŒ“ê¸€ê°ì •ë¶„ì„_GPT_labeled.xlsx\")\n",
    "\n",
    "    posts = pd.read_excel(post_path)\n",
    "    comments = pd.read_excel(cmt_path)\n",
    "\n",
    "    # ë³‘í•© í‚¤ í†µì¼ (URL ê¸°ì¤€)\n",
    "    posts[\"videoId\"] = posts[\"url\"]\n",
    "    comments[\"videoId\"] = comments[\"url\"]\n",
    "\n",
    "    # âœ… ëŒ“ê¸€ ìˆ˜ ê³„ì‚° (videoId ê¸°ì¤€)\n",
    "    comment_counts = comments.groupby(\"videoId\").size().rename(\"comment_cnt\").reset_index()\n",
    "\n",
    "    # âœ… ê²Œì‹œê¸€ì— ëŒ“ê¸€ìˆ˜ ì¶”ê°€ (ì—†ìœ¼ë©´ 0)\n",
    "    posts = posts.merge(comment_counts, on=\"videoId\", how=\"left\").fillna({\"comment_cnt\": 0})\n",
    "    posts[\"comment_cnt\"] = posts[\"comment_cnt\"].astype(int)\n",
    "\n",
    "    # âœ… ì „ì²´ ëŒ“ê¸€ ìˆ˜ ì¹´ìš´íŠ¸\n",
    "    total_comments = comments.shape[0]\n",
    "\n",
    "    # âœ… ê²°ê³¼ ì €ì¥\n",
    "    output_path = os.path.join(BASE_PATH, f\"{issue}_posts_with_comment_cnt.xlsx\")\n",
    "    posts.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} ì™„ë£Œ â€” ê²Œì‹œê¸€ {len(posts)}ê±´, ëŒ“ê¸€ìˆ˜ ì¶”ê°€ í›„ ì €ì¥ë¨\")\n",
    "    print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {output_path}\")\n",
    "    print(f\"ğŸ’¬ {issue} ì´ìŠˆ ëŒ“ê¸€ ì´ {total_comments:,}ê°œê°€ ì¶”ê°€ ë° ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ ì‹¤í–‰\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    add_comment_count_to_posts(issue)\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ ê²Œì‹œê¸€ì— ëŒ“ê¸€ìˆ˜ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ GS ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸš€ KT ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸš€ ë¡¯ë° ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸš€ SKT ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 1. ê¸°ë³¸ ì„¤ì •\n",
    "# ============================================================\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ê³µìœ í´ë”(ë”¥í…Œí¬)\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 2. í•¨ìˆ˜ ì •ì˜\n",
    "# ============================================================\n",
    "\n",
    "def load_data(issue):\n",
    "    \"\"\"posts / comments ë°ì´í„° ë¡œë“œ + ë‚ ì§œ ì²˜ë¦¬\"\"\"\n",
    "    posts = pd.read_excel(os.path.join(BASE_PATH, f\"{issue}_posts_with_comment_cnt.xlsx\"))\n",
    "    comments = pd.read_excel(os.path.join(BASE_PATH, f\"{issue}_ëŒ“ê¸€ê°ì •ë¶„ì„_GPT_labeled.xlsx\"))\n",
    "    \n",
    "    posts[\"publishedAt\"] = pd.to_datetime(posts[\"ê²Œì‹œì¼\"], errors=\"coerce\", utc=True)\n",
    "    comments[\"publishedAt\"] = pd.to_datetime(comments[\"post_date\"], errors=\"coerce\", utc=True)\n",
    "    \n",
    "    return posts, comments\n",
    "\n",
    "\n",
    "def merge_post_comment(posts, comments):\n",
    "    \"\"\"ê²Œì‹œê¸€ê³¼ ëŒ“ê¸€ ë°ì´í„° ë³‘í•© + ì‹œê°„ì°¨ ê³„ì‚°\"\"\"\n",
    "    merged = pd.merge(\n",
    "        posts,\n",
    "        comments[[\"url\", \"publishedAt\", \"gpt_sentiment\"]],\n",
    "        on=\"url\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_post\", \"_comment\")\n",
    "    )\n",
    "\n",
    "    merged[\"hours_after\"] = (\n",
    "        (merged[\"publishedAt_comment\"] - merged[\"publishedAt_post\"]).dt.total_seconds() / 3600\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "\n",
    "def calc_alignment(merged):\n",
    "    \"\"\"ê²Œì‹œê¸€-ëŒ“ê¸€ ê°ì • ì •ë ¬ë„ ê³„ì‚°\"\"\"\n",
    "    def to_score(x):\n",
    "        if isinstance(x, str):\n",
    "            if \"ë¶€ì •\" in x:\n",
    "                return -1\n",
    "            elif \"ê¸ì •\" in x:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    merged[\"post_s\"] = merged[\"gpt_sentiment_post\"].apply(to_score)\n",
    "    merged[\"cmt_s\"] = merged[\"gpt_sentiment_comment\"].apply(to_score)\n",
    "\n",
    "    # videoId ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ url ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”\n",
    "    grouped = (\n",
    "        merged.groupby(\"url\")\n",
    "        .agg(\n",
    "            í‚¤ì›Œë“œ=(\"í‚¤ì›Œë“œ\", \"first\"),\n",
    "            ì–¸ë¡ ì‚¬=(\"ì–¸ë¡ ì‚¬\", \"first\"),\n",
    "            ë‰´ìŠ¤ì œëª©=(\"ë‰´ìŠ¤ì œëª©\", \"first\"),\n",
    "            ê²Œì‹œì¼=(\"ê²Œì‹œì¼\", \"first\"),\n",
    "            post_sentiment=(\"gpt_sentiment_post\", \"first\"),\n",
    "            post_s=(\"post_s\", \"first\"),\n",
    "            cmt_s_mean=(\"cmt_s\", \"mean\"),\n",
    "            comment_cnt=(\"comment_cnt\", \"first\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # ê°ì • ì°¨ì´\n",
    "    grouped[\"abs_sent_gap\"] = (grouped[\"post_s\"] - grouped[\"cmt_s_mean\"]).abs().round(2)\n",
    "\n",
    "    # 3/6/9ì‹œê°„ ë‚´ ë™ì¼ê°ì • ëŒ“ê¸€ ìˆ˜ ì¹´ìš´íŠ¸\n",
    "    for h in [3, 6, 9]:\n",
    "        cond = (merged[\"hours_after\"] <= h) & (merged[\"post_s\"] == merged[\"cmt_s\"])\n",
    "        aligned = merged[cond].groupby(\"url\").size()\n",
    "        grouped[f\"aligned_cnt_{h}h\"] = grouped[\"url\"].map(aligned).fillna(0).astype(int)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… 3. ì‹¤í–‰ ë£¨í”„\n",
    "# ============================================================\n",
    "for issue in ISSUES:\n",
    "    print(f\"\\nğŸš€ {issue} ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "    posts, comments = load_data(issue)\n",
    "    merged = merge_post_comment(posts, comments)\n",
    "    result = calc_alignment(merged)\n",
    "\n",
    "    # Excel ì €ì¥ ì „ timezone ì œê±°\n",
    "    for col in merged.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "        merged[col] = merged[col].dt.tz_localize(None)\n",
    "    for col in result.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "        result[col] = result[col].dt.tz_localize(None)\n",
    "\n",
    "    # ì €ì¥\n",
    "    merged.to_excel(os.path.join(BASE_PATH, f\"{issue}_merged_timegap.xlsx\"), index=False, engine=\"openpyxl\")\n",
    "    result.to_excel(os.path.join(BASE_PATH, f\"{issue}_final_post_level.xlsx\"), index=False, engine=\"openpyxl\")\n",
    "\n",
    "    \n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ ì²˜ë¦¬ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d724ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS ë³‘í•© ì™„ë£Œ â€” 292í–‰ ì €ì¥\n",
      "ğŸ“ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„.xlsx\n",
      "âœ… KT ë³‘í•© ì™„ë£Œ â€” 934í–‰ ì €ì¥\n",
      "ğŸ“ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„.xlsx\n",
      "âœ… ë¡¯ë° ë³‘í•© ì™„ë£Œ â€” 1,181í–‰ ì €ì¥\n",
      "ğŸ“ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„.xlsx\n",
      "âœ… SKT ë³‘í•© ì™„ë£Œ â€” 2,575í–‰ ì €ì¥\n",
      "ğŸ“ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„.xlsx\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ë° ì¶”ê°€ì±„ë„ ë³‘í•© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê²½ë¡œ ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ë‚ ì§œì‹œê°„\"\n",
    "ADD_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\ë°ì´í„°\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ì´ìŠˆë³„ ë³‘í•© ë£¨í”„\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    base_file = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final.xlsx\")\n",
    "    add_file = os.path.join(ADD_PATH, f\"merged_output_{issue.lower()}.xlsx\")\n",
    "    output_file = os.path.join(OUTPUT_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„.xlsx\")\n",
    "\n",
    "    if not os.path.exists(base_file):\n",
    "        print(f\"âš ï¸ {issue}: ë² ì´ìŠ¤ íŒŒì¼ ì—†ìŒ â†’ {base_file}\")\n",
    "        continue\n",
    "    if not os.path.exists(add_file):\n",
    "        print(f\"âš ï¸ {issue}: ì¶”ê°€ íŒŒì¼ ì—†ìŒ â†’ {add_file}\")\n",
    "        continue\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    # --------------------------------------------------------\n",
    "    base_df = pd.read_excel(base_file)\n",
    "    add_df = pd.read_excel(add_file)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4ï¸âƒ£ ì»¬ëŸ¼ëª… í†µì¼\n",
    "    # --------------------------------------------------------\n",
    "    add_df = add_df.rename(columns={\n",
    "        \"date\": \"datetime\",\n",
    "        \"title\": \"channelTitle\",\n",
    "        \"text\": \"title_or_text\"\n",
    "    })\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5ï¸âƒ£ ë‚ ì§œ ë³€í™˜\n",
    "    # --------------------------------------------------------\n",
    "    base_df[\"datetime\"] = pd.to_datetime(base_df[\"datetime\"], errors=\"coerce\")\n",
    "    add_df[\"datetime\"] = pd.to_datetime(add_df[\"datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6ï¸âƒ£ ë³‘í•© (outer join)\n",
    "    # --------------------------------------------------------\n",
    "    merged = pd.merge(\n",
    "        base_df,\n",
    "        add_df[[\"datetime\", \"channel\", \"channelTitle\", \"title_or_text\"]],\n",
    "        on=\"datetime\",\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_base\", \"_add\")\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 7ï¸âƒ£ ì €ì¥\n",
    "    # --------------------------------------------------------\n",
    "    merged.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} ë³‘í•© ì™„ë£Œ â€” {len(merged):,}í–‰ ì €ì¥\")\n",
    "    print(f\"ğŸ“ {output_file}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ í†µí•© ë° ì¶”ê°€ì±„ë„ ë³‘í•© ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b69655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê²½ë¡œ ë° ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì •ë¦¬ì™„ë£Œ\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ë£¨í”„ ì‹œì‘\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    file_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„.xlsx\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ {issue} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3ï¸âƒ£ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    # --------------------------------------------------------\n",
    "    cols = df.columns.tolist()\n",
    "    print(f\"\\nğŸ“‚ {issue} ì»¬ëŸ¼: {cols}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4ï¸âƒ£ ì±„ë„ ì»¬ëŸ¼ í†µì¼\n",
    "    # --------------------------------------------------------\n",
    "    if \"channel_base\" in df.columns and \"channel_add\" in df.columns:\n",
    "        df[\"channel\"] = df[\"channel_add\"].combine_first(df[\"channel_base\"])\n",
    "    elif \"channel_add\" in df.columns:\n",
    "        df[\"channel\"] = df[\"channel_add\"]\n",
    "    elif \"channel_base\" in df.columns:\n",
    "        df[\"channel\"] = df[\"channel_base\"]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5ï¸âƒ£ ì œëª©Â·ë³¸ë¬¸ ì»¬ëŸ¼ í†µí•©\n",
    "    # --------------------------------------------------------\n",
    "    title_cols = [c for c in df.columns if \"title_or_text\" in c or \"channelTitle\" in c]\n",
    "    df[\"title_or_text\"] = df.get(\"title_or_text_add\", \"\").astype(str).fillna(\"\") + \" \" + \\\n",
    "                          df.get(\"channelTitle_add\", \"\").astype(str).fillna(\"\")\n",
    "    # ë§Œì•½ title_or_text_baseê°€ ìˆë‹¤ë©´ ìš°ì„  ì‚¬ìš©í•˜ê³ , ë¹„ì–´ìˆìœ¼ë©´ í†µí•©ë³¸ ì‚¬ìš©\n",
    "    if \"title_or_text_base\" in df.columns:\n",
    "        df[\"title_or_text\"] = df[\"title_or_text_base\"].combine_first(df[\"title_or_text\"])\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6ï¸âƒ£ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (_base, _add)\n",
    "    # --------------------------------------------------------\n",
    "    df = df[[c for c in df.columns if not (c.endswith(\"_base\") or c.endswith(\"_add\"))]]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 7ï¸âƒ£ ì •ë¦¬ ë° ì €ì¥\n",
    "    # --------------------------------------------------------\n",
    "    output_file = os.path.join(OUTPUT_PATH, f\"{issue}_í†µí•©_final_ì •ë¦¬ì™„ë£Œ.xlsx\")\n",
    "    df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} ì •ë¦¬ ì™„ë£Œ ({len(df):,}í–‰ ì €ì¥)\")\n",
    "    print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ ì •ë¦¬ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9ff62",
   "metadata": {},
   "source": [
    "+íŠ¸ìœ„í„°ì™€ ë¸”ë¡œê·¸ ê°ì •ìˆ˜í–‰ì„ ì•ˆí•´ì„œ ìµœì¢…ë³¸ì— ê°ì •ë¶„ì„ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e7a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tqdm pandas --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79478141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ GS ê°ì •ë¶„ì„ ì‹œì‘ ì¤‘...\n",
      "ğŸ“‚ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°: 292í–‰\n",
      "ğŸ§© ê°ì •ë¶„ì„ ëŒ€ìƒ: 84ê±´ (blog/twitter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT ê°ì •ë¶„ì„ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:43<00:00,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS ê°ì •ë¶„ì„ ì™„ë£Œ!\n",
      "ğŸ“ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "ğŸ“Š blog/twitter ê°ì •ë¶„ì„ ì™„ë£Œ í–‰ ìˆ˜: 84\n",
      "\n",
      "ğŸš€ KT ê°ì •ë¶„ì„ ì‹œì‘ ì¤‘...\n",
      "ğŸ“‚ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°: 934í–‰\n",
      "ğŸ§© ê°ì •ë¶„ì„ ëŒ€ìƒ: 149ê±´ (blog/twitter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT ê°ì •ë¶„ì„ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:03<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KT ê°ì •ë¶„ì„ ì™„ë£Œ!\n",
      "ğŸ“ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "ğŸ“Š blog/twitter ê°ì •ë¶„ì„ ì™„ë£Œ í–‰ ìˆ˜: 149\n",
      "\n",
      "ğŸš€ ë¡¯ë° ê°ì •ë¶„ì„ ì‹œì‘ ì¤‘...\n",
      "ğŸ“‚ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°: 1,181í–‰\n",
      "ğŸ§© ê°ì •ë¶„ì„ ëŒ€ìƒ: 342ê±´ (blog/twitter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT ê°ì •ë¶„ì„ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [02:21<00:00,  7.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¡¯ë° ê°ì •ë¶„ì„ ì™„ë£Œ!\n",
      "ğŸ“ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "ğŸ“Š blog/twitter ê°ì •ë¶„ì„ ì™„ë£Œ í–‰ ìˆ˜: 342\n",
      "\n",
      "ğŸš€ SKT ê°ì •ë¶„ì„ ì‹œì‘ ì¤‘...\n",
      "ğŸ“‚ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°: 2,575í–‰\n",
      "ğŸ§© ê°ì •ë¶„ì„ ëŒ€ìƒ: 1,202ê±´ (blog/twitter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT ê°ì •ë¶„ì„ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [08:44<00:00,  8.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SKT ê°ì •ë¶„ì„ ì™„ë£Œ!\n",
      "ğŸ“ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "ğŸ“Š blog/twitter ê°ì •ë¶„ì„ ì™„ë£Œ í–‰ ìˆ˜: 1,202\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ìŠˆ ê°ì •ë¶„ì„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# âœ… ë¸”ë¡œê·¸Â·íŠ¸ìœ„í„° ì „ìš© GPT ê°ì •ë¶„ì„ (PoC í†µí•©ë°ì´í„° 4ì´ìŠˆ ìë™)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import json, time, re, os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# ğŸ”‘ OpenAI API í‚¤\n",
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
    "client = OpenAI(api_key=api_key)",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ GPT ê°ì •ë¶„ì„ í•¨ìˆ˜ (ë°°ì¹˜ + ìºì‹±)\n",
    "# ------------------------------------------------------------\n",
    "cache = {}\n",
    "\n",
    "def gpt_batch_sentiment(texts, batch_size=20, sleep_sec=0.5):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ GPT-4o-minië¡œ ë°°ì¹˜ ê°ì •ë¶„ì„\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"GPT ê°ì •ë¶„ì„ ì¤‘\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        new_texts = [t for t in batch if t not in cache]\n",
    "\n",
    "        # âœ… ìºì‹œì— ëª¨ë‘ ìˆìœ¼ë©´ ë°”ë¡œ append\n",
    "        if not new_texts:\n",
    "            results.extend([cache[t] for t in batch])\n",
    "            continue\n",
    "\n",
    "        # âœ… í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        batch_prompt = \"\\n\".join([f\"{j+1}. {t}\" for j, t in enumerate(new_texts, start=1)])\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¤ìŒ ë¬¸ì¥ë“¤ì˜ ê°ì •ì„ JSON ë°°ì—´ë¡œ ì¶œë ¥í•´.\n",
    "        ê° ë¬¸ì¥ì€ ë°˜ë“œì‹œ 'sentiment'(ë¶€ì •/ì¤‘ë¦½/ê¸ì •)ì™€ 'confidence'(0~1)ë¥¼ í¬í•¨í•´ì•¼ í•´.\n",
    "\n",
    "        ì˜ˆì‹œ:\n",
    "        [\n",
    "          {{\"id\":1, \"sentiment\":\"ë¶€ì •\", \"confidence\":0.92}},\n",
    "          {{\"id\":2, \"sentiment\":\"ì¤‘ë¦½\", \"confidence\":0.53}}\n",
    "        ]\n",
    "\n",
    "        ë¬¸ì¥:\n",
    "        {batch_prompt}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # âœ… GPT í˜¸ì¶œ\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"ë„ˆëŠ” í•œêµ­ì–´ ê°ì •ë¶„ì„ ì „ë¬¸ê°€ì•¼.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "\n",
    "            output = response.choices[0].message.content.strip()\n",
    "            match = re.search(r\"\\[.*\\]\", output, re.S)\n",
    "            parsed = json.loads(match.group(0)) if match else []\n",
    "\n",
    "            # âœ… ê²°ê³¼ ì •ë¦¬ ë° ìºì‹±\n",
    "            for j, t in enumerate(new_texts, start=1):\n",
    "                if j <= len(parsed):\n",
    "                    s = parsed[j-1].get(\"sentiment\", \"ì¤‘ë¦½\")\n",
    "                    c = parsed[j-1].get(\"confidence\", 0.5)\n",
    "                else:\n",
    "                    s, c = \"ì¤‘ë¦½\", 0.0\n",
    "                cache[t] = {\"sentiment\": s, \"confidence\": c}\n",
    "\n",
    "            results.extend([cache[t] for t in batch])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            results.extend([{\"sentiment\": \"ì¤‘ë¦½\", \"confidence\": 0.0} for _ in batch])\n",
    "\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ê²½ë¡œ ë° ì´ìŠˆ ëª©ë¡\n",
    "# ------------------------------------------------------------\n",
    "INPUT_DIR = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì •ë¦¬ì™„ë£Œ\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ ì´ìŠˆë³„ ê°ì •ë¶„ì„ ì‹¤í–‰ ë£¨í”„\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    print(f\"\\nğŸš€ {issue} ê°ì •ë¶„ì„ ì‹œì‘ ì¤‘...\")\n",
    "\n",
    "    input_path = fr\"{INPUT_DIR}\\{issue}_í†µí•©_final_ì •ë¦¬ì™„ë£Œ.xlsx\"\n",
    "    output_path = fr\"{OUTPUT_DIR}\\{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\"\n",
    "\n",
    "    # âœ… íŒŒì¼ ìœ íš¨ì„± í™•ì¸\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"âš ï¸ {issue} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ â†’ {input_path}\")\n",
    "        continue\n",
    "\n",
    "    # âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    df = pd.read_excel(input_path)\n",
    "    print(f\"ğŸ“‚ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°: {len(df):,}í–‰\")\n",
    "\n",
    "    # âœ… blog / twitter ëŒ€ìƒ í•„í„°ë§\n",
    "    if \"channel\" not in df.columns or \"title_or_text\" not in df.columns:\n",
    "        print(f\"âš ï¸ {issue} íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤ (channel, title_or_text)\")\n",
    "        continue\n",
    "\n",
    "    mask = df[\"channel\"].astype(str).str.lower().isin([\"blog\", \"twitter\"])\n",
    "    target_df = df[mask].copy()\n",
    "\n",
    "    print(f\"ğŸ§© ê°ì •ë¶„ì„ ëŒ€ìƒ: {len(target_df):,}ê±´ (blog/twitter)\")\n",
    "\n",
    "    # âœ… ê°ì •ë¶„ì„ ì‹¤í–‰\n",
    "    if len(target_df) == 0:\n",
    "        print(f\"âš ï¸ {issue}ì—ëŠ” blog/twitter ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.\")\n",
    "        continue\n",
    "\n",
    "    texts = target_df[\"title_or_text\"].astype(str).fillna(\"\").str.strip().str[:300].tolist()\n",
    "    results = gpt_batch_sentiment(texts, batch_size=20)\n",
    "\n",
    "    target_df[\"post_sentiment\"] = [r[\"sentiment\"] for r in results]\n",
    "    target_df[\"gpt_confidence\"] = [r[\"confidence\"] for r in results]\n",
    "\n",
    "    # âœ… ì›ë³¸ ë³‘í•© (í•´ë‹¹ ì¸ë±ìŠ¤ë§Œ ì—…ë°ì´íŠ¸)\n",
    "    df.loc[target_df.index, \"post_sentiment\"] = target_df[\"post_sentiment\"]\n",
    "    df.loc[target_df.index, \"gpt_confidence\"] = target_df[\"gpt_confidence\"]\n",
    "\n",
    "    # âœ… ê²°ê³¼ ì €ì¥\n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… {issue} ê°ì •ë¶„ì„ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ ì €ì¥ ê²½ë¡œ: {output_path}\")\n",
    "    print(f\"ğŸ“Š blog/twitter ê°ì •ë¶„ì„ ì™„ë£Œ í–‰ ìˆ˜: {len(target_df):,}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  ì´ìŠˆ ê°ì •ë¶„ì„ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f307a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}