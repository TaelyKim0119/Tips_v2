{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸ“‚ GS íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ë°ì´í„° í¬ê¸°: 292í–‰ Ã— 18ì—´\n",
      "ğŸ“‹ ì£¼ìš” ì»¬ëŸ¼: ['datetime', 'content_id', 'post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h', 'viewCount'] ...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 292 entries, 0 to 291\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   datetime         292 non-null    datetime64[ns]\n",
      " 1   content_id       208 non-null    object        \n",
      " 2   post_sentiment   292 non-null    object        \n",
      " 3   post_s           208 non-null    float64       \n",
      " 4   cmt_s_mean       208 non-null    float64       \n",
      " 5   abs_sent_gap     208 non-null    float64       \n",
      " 6   aligned_cnt_3h   208 non-null    float64       \n",
      " 7   aligned_cnt_6h   208 non-null    float64       \n",
      " 8   aligned_cnt_9h   208 non-null    float64       \n",
      " 9   viewCount        22 non-null     float64       \n",
      " 10  likeCount        22 non-null     float64       \n",
      " 11  comment_cnt      208 non-null    float64       \n",
      " 12  subscriberCount  22 non-null     float64       \n",
      " 13  ì–¸ë¡ ì‚¬              186 non-null    object        \n",
      " 14  content          186 non-null    object        \n",
      " 15  channel          292 non-null    object        \n",
      " 16  title_or_text    292 non-null    object        \n",
      " 17  gpt_confidence   84 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(11), object(6)\n",
      "memory usage: 41.2+ KB\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ KT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ë°ì´í„° í¬ê¸°: 934í–‰ Ã— 18ì—´\n",
      "ğŸ“‹ ì£¼ìš” ì»¬ëŸ¼: ['datetime', 'content_id', 'post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h', 'viewCount'] ...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 934 entries, 0 to 933\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   datetime         934 non-null    datetime64[ns]\n",
      " 1   content_id       785 non-null    object        \n",
      " 2   post_sentiment   934 non-null    object        \n",
      " 3   post_s           785 non-null    float64       \n",
      " 4   cmt_s_mean       785 non-null    float64       \n",
      " 5   abs_sent_gap     785 non-null    float64       \n",
      " 6   aligned_cnt_3h   785 non-null    float64       \n",
      " 7   aligned_cnt_6h   785 non-null    float64       \n",
      " 8   aligned_cnt_9h   785 non-null    float64       \n",
      " 9   viewCount        349 non-null    float64       \n",
      " 10  likeCount        349 non-null    float64       \n",
      " 11  comment_cnt      785 non-null    float64       \n",
      " 12  subscriberCount  349 non-null    float64       \n",
      " 13  ì–¸ë¡ ì‚¬              436 non-null    object        \n",
      " 14  content          436 non-null    object        \n",
      " 15  channel          934 non-null    object        \n",
      " 16  title_or_text    934 non-null    object        \n",
      " 17  gpt_confidence   149 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(11), object(6)\n",
      "memory usage: 131.5+ KB\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ ë¡¯ë° íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ë°ì´í„° í¬ê¸°: 1,181í–‰ Ã— 18ì—´\n",
      "ğŸ“‹ ì£¼ìš” ì»¬ëŸ¼: ['datetime', 'content_id', 'post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h', 'viewCount'] ...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1181 entries, 0 to 1180\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   datetime         1181 non-null   datetime64[ns]\n",
      " 1   content_id       840 non-null    object        \n",
      " 2   post_sentiment   1181 non-null   object        \n",
      " 3   post_s           840 non-null    float64       \n",
      " 4   cmt_s_mean       840 non-null    float64       \n",
      " 5   abs_sent_gap     840 non-null    float64       \n",
      " 6   aligned_cnt_3h   840 non-null    float64       \n",
      " 7   aligned_cnt_6h   840 non-null    float64       \n",
      " 8   aligned_cnt_9h   840 non-null    float64       \n",
      " 9   viewCount        500 non-null    float64       \n",
      " 10  likeCount        500 non-null    float64       \n",
      " 11  comment_cnt      840 non-null    float64       \n",
      " 12  subscriberCount  500 non-null    float64       \n",
      " 13  ì–¸ë¡ ì‚¬              340 non-null    object        \n",
      " 14  content          339 non-null    object        \n",
      " 15  channel          1181 non-null   object        \n",
      " 16  title_or_text    1181 non-null   object        \n",
      " 17  gpt_confidence   342 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(11), object(6)\n",
      "memory usage: 166.2+ KB\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ SKT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ë°ì´í„° í¬ê¸°: 2,575í–‰ Ã— 18ì—´\n",
      "ğŸ“‹ ì£¼ìš” ì»¬ëŸ¼: ['datetime', 'content_id', 'post_sentiment', 'post_s', 'cmt_s_mean', 'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h', 'viewCount'] ...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2575 entries, 0 to 2574\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   datetime         2575 non-null   datetime64[ns]\n",
      " 1   content_id       1375 non-null   object        \n",
      " 2   post_sentiment   2575 non-null   object        \n",
      " 3   post_s           1375 non-null   float64       \n",
      " 4   cmt_s_mean       1375 non-null   float64       \n",
      " 5   abs_sent_gap     1375 non-null   float64       \n",
      " 6   aligned_cnt_3h   1375 non-null   float64       \n",
      " 7   aligned_cnt_6h   1375 non-null   float64       \n",
      " 8   aligned_cnt_9h   1375 non-null   float64       \n",
      " 9   viewCount        494 non-null    float64       \n",
      " 10  likeCount        494 non-null    float64       \n",
      " 11  comment_cnt      1375 non-null   float64       \n",
      " 12  subscriberCount  494 non-null    float64       \n",
      " 13  ì–¸ë¡ ì‚¬              881 non-null    object        \n",
      " 14  content          880 non-null    object        \n",
      " 15  channel          2575 non-null   object        \n",
      " 16  title_or_text    2575 non-null   object        \n",
      " 17  gpt_confidence   1202 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(11), object(6)\n",
      "memory usage: 362.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\"  # ìœ„ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# 2ï¸âƒ£ íŒŒì¼ ì—´ê¸° ë° info ì¶œë ¥\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    file_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"ğŸ“‚ {issue} íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}\")\n",
    "    print(f\"==============================\")\n",
    "    \n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"âœ… ë°ì´í„° í¬ê¸°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]:,}ì—´\")\n",
    "    print(f\"ğŸ“‹ ì£¼ìš” ì»¬ëŸ¼: {list(df.columns[:10])} ...\")\n",
    "    print()\n",
    "    df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸ“‚ GS íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 292 â†’ 288 (ì‚­ì œëœ ì¤‘ë³µ 4ê±´)\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ KT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 934 â†’ 934 (ì‚­ì œëœ ì¤‘ë³µ 0ê±´)\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ ë¡¯ë° íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 1,181 â†’ 1,180 (ì‚­ì œëœ ì¤‘ë³µ 1ê±´)\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ SKT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 2,575 â†’ 2,573 (ì‚­ì œëœ ì¤‘ë³µ 2ê±´)\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ íŒŒì¼ë³„ ì¤‘ë³µ ì œê±° ë° ì €ì¥\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    file_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ê°ì •ë¶„ì„ì™„ë£Œ.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"ğŸ“‚ {issue} íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    df = pd.read_excel(file_path)\n",
    "    before = len(df)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âœ… ì¤‘ë³µ ì œê±° (ìš°ì„  content_id + title_or_text ê¸°ì¤€)\n",
    "    # --------------------------------------------------------\n",
    "    if \"content_id\" in df.columns and \"title_or_text\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\", \"title_or_text\"], keep=\"first\")\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset=[\"datetime\", \"channel\", \"title_or_text\"], keep=\"first\")\n",
    "\n",
    "    after = len(df)\n",
    "    removed = before - after\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âœ… ì €ì¥\n",
    "    # --------------------------------------------------------\n",
    "    save_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\")\n",
    "    df.to_excel(save_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {before:,} â†’ {after:,} (ì‚­ì œëœ ì¤‘ë³µ {removed:,}ê±´)\")\n",
    "    print(f\"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸ“‚ GS íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 288 â†’ 288\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ KT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 934 â†’ 934\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ ë¡¯ë° íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 1,179 â†’ 1,179\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ SKT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 2,573 â†’ 2,573\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ì¤‘ë³µì œê±° + content_id ê°±ì‹  + gpt_confidence ì œê±°\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    file_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"ğŸ“‚ {issue} íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "    before = len(df)\n",
    "\n",
    "    # âœ… 1) ì¤‘ë³µ ì œê±°\n",
    "    if \"content_id\" in df.columns and \"title_or_text\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\", \"title_or_text\"], keep=\"first\")\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset=[\"datetime\", \"channel\", \"title_or_text\"], keep=\"first\")\n",
    "\n",
    "    after = len(df)\n",
    "    print(f\"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {before:,} â†’ {after:,}\")\n",
    "\n",
    "    # âœ… 2) gpt_confidence ì»¬ëŸ¼ ì œê±° (ì¡´ì¬ ì‹œë§Œ)\n",
    "    if \"gpt_confidence\" in df.columns:\n",
    "        df = df.drop(columns=[\"gpt_confidence\"])\n",
    "        print(\"ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\")\n",
    "\n",
    "    # âœ… 3) content_id ì´ˆê¸°í™” â†’ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¡œ ì¬ì§€ì •\n",
    "    if \"content_id\" in df.columns:\n",
    "        df.drop(columns=[\"content_id\"], inplace=True)\n",
    "    df.insert(0, \"content_id\", range(1, len(df) + 1))  # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ë²ˆí˜¸\n",
    "\n",
    "    print(\"ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "    # âœ… 4) ì €ì¥\n",
    "    save_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\")\n",
    "    df.to_excel(save_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸ“‚ GS íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 288 â†’ 288\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ KT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 934 â†’ 934\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ ë¡¯ë° íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 1,180 â†’ 1,180\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n",
      "\n",
      "==============================\n",
      "ğŸ“‚ SKT íŒŒì¼ ë¡œë“œ ì¤‘: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\n",
      "==============================\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: 2,573 â†’ 2,573\n",
      "ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ì¤‘ë³µì œê±° + content_id ê°±ì‹  + gpt_confidence ì œê±°\n",
    "# ------------------------------------------------------------\n",
    "for issue in ISSUES:\n",
    "    file_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ì¤‘ë³µì œê±°.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"ğŸ“‚ {issue} íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "    before = len(df)\n",
    "\n",
    "    # âœ… 1) ì¤‘ë³µ ì œê±°\n",
    "    if \"content_id\" in df.columns and \"title_or_text\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\", \"title_or_text\"], keep=\"first\")\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset=[\"datetime\", \"channel\", \"title_or_text\"], keep=\"first\")\n",
    "\n",
    "    after = len(df)\n",
    "    print(f\"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {before:,} â†’ {after:,}\")\n",
    "\n",
    "    # âœ… 2) gpt_confidence ì»¬ëŸ¼ ì œê±° (ì¡´ì¬ ì‹œë§Œ)\n",
    "    if \"gpt_confidence\" in df.columns:\n",
    "        df = df.drop(columns=[\"gpt_confidence\"])\n",
    "        print(\"ğŸ§¹ gpt_confidence ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\")\n",
    "\n",
    "    # âœ… 3) content_id ì´ˆê¸°í™” â†’ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¡œ ì¬ì§€ì •\n",
    "    if \"content_id\" in df.columns:\n",
    "        df.drop(columns=[\"content_id\"], inplace=True)\n",
    "    df.insert(0, \"content_id\", range(1, len(df) + 1))  # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ë²ˆí˜¸\n",
    "\n",
    "    print(\"ğŸ”¢ content_id ì¸ë±ìŠ¤ë¡œ ì¬ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "    # âœ… 4) ì €ì¥\n",
    "    save_path = os.path.join(BASE_PATH, f\"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\")\n",
    "    df.to_excel(save_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    print(f\"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content_id', 'datetime', 'post_sentiment', 'post_s', 'cmt_s_mean',\n",
       "       'abs_sent_gap', 'aligned_cnt_3h', 'aligned_cnt_6h', 'aligned_cnt_9h',\n",
       "       'viewCount', 'likeCount', 'comment_cnt', 'subscriberCount', 'ì–¸ë¡ ì‚¬',\n",
       "       'content', 'channel', 'title_or_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ GS ì²˜ë¦¬ ì¤‘... â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GS í˜•íƒœì†Œ ë¶„ì„+ë¼ë²¨ ë§¤ì¹­ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [00:01<00:00, 159.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GS ì™„ë£Œ:\n",
      "   ì „ì²´ ê¸°ì‚¬: 288ê±´\n",
      "   ë§¤ì¹­ëœ ê¸°ì‚¬: 267ê±´\n",
      "   ì œê±°ëœ ê¸°ì‚¬: 21ê±´\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\GS_í˜•íƒœì†Œë¼ë²¨_ë¯¸ë§¤ì¹­ì œê±°ì™„ë£Œ.xlsx\n",
      "\n",
      "ğŸ“‚ KT ì²˜ë¦¬ ì¤‘... â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KT í˜•íƒœì†Œ ë¶„ì„+ë¼ë²¨ ë§¤ì¹­ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 934/934 [00:02<00:00, 355.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KT ì™„ë£Œ:\n",
      "   ì „ì²´ ê¸°ì‚¬: 934ê±´\n",
      "   ë§¤ì¹­ëœ ê¸°ì‚¬: 924ê±´\n",
      "   ì œê±°ëœ ê¸°ì‚¬: 10ê±´\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\KT_í˜•íƒœì†Œë¼ë²¨_ë¯¸ë§¤ì¹­ì œê±°ì™„ë£Œ.xlsx\n",
      "\n",
      "ğŸ“‚ ë¡¯ë° ì²˜ë¦¬ ì¤‘... â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë¡¯ë° í˜•íƒœì†Œ ë¶„ì„+ë¼ë²¨ ë§¤ì¹­ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1180/1180 [00:04<00:00, 287.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¡¯ë° ì™„ë£Œ:\n",
      "   ì „ì²´ ê¸°ì‚¬: 1,180ê±´\n",
      "   ë§¤ì¹­ëœ ê¸°ì‚¬: 1,173ê±´\n",
      "   ì œê±°ëœ ê¸°ì‚¬: 7ê±´\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ë¡¯ë°_í˜•íƒœì†Œë¼ë²¨_ë¯¸ë§¤ì¹­ì œê±°ì™„ë£Œ.xlsx\n",
      "\n",
      "ğŸ“‚ SKT ì²˜ë¦¬ ì¤‘... â†’ C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKT í˜•íƒœì†Œ ë¶„ì„+ë¼ë²¨ ë§¤ì¹­ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2573/2573 [00:22<00:00, 113.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SKT ì™„ë£Œ:\n",
      "   ì „ì²´ ê¸°ì‚¬: 2,573ê±´\n",
      "   ë§¤ì¹­ëœ ê¸°ì‚¬: 2,561ê±´\n",
      "   ì œê±°ëœ ê¸°ì‚¬: 12ê±´\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\SKT_í˜•íƒœì†Œë¼ë²¨_ë¯¸ë§¤ì¹­ì œê±°ì™„ë£Œ.xlsx\n",
      "\n",
      "ğŸ“Š ë ‰ì‹œì½˜ ë§¤ì¹­ ìš”ì•½ (ì´ìŠˆë³„)\n",
      "| ì´ìŠˆ   |   ì „ì²´ê¸°ì‚¬ |   ë§¤ì¹­ëœê¸°ì‚¬ |   ì œê±°ëœê¸°ì‚¬ |   ë§¤ì¹­ë¹„ìœ¨(%) |\n",
      "|:-------|-----------:|-------------:|-------------:|--------------:|\n",
      "| GS     |        288 |          267 |           21 |          92.7 |\n",
      "| KT     |        934 |          924 |           10 |          98.9 |\n",
      "| ë¡¯ë°   |       1180 |         1173 |            7 |          99.4 |\n",
      "| SKT    |       2573 |         2561 |           12 |          99.5 |\n",
      "\n",
      "ğŸ’¾ ìš”ì•½ ì €ì¥ ì™„ë£Œ: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\LEXICON_ë§¤ì¹­ìš”ì•½.xlsx\n",
      "ğŸ’¾ ë‹¨ì–´ ë¹ˆë„ ìš”ì•½ ì €ì¥: C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\LEXICON_ë‹¨ì–´ë¹ˆë„ìš”ì•½.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ + ìµœì‹  ë ‰ì‹œì½˜ ê¸°ë°˜ ë¼ë²¨ ì¹´ìš´íŠ¸\n",
    "#    + ë¯¸ë§¤ì¹­ ì œê±° + content_id ë¶€ì—¬ + ë§¤ì¹­ë‹¨ì–´ ëª©ë¡ ì €ì¥\n",
    "#    + ì´ìŠˆë³„ ë‹¨ì–´ ë¹ˆë„ ìš”ì•½ ì €ì¥ (ì˜µì…˜)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from kiwipiepy import Kiwi\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "BASE_PATH = r\"C:\\Users\\user\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\"\n",
    "ISSUES = [\"GS\", \"KT\", \"ë¡¯ë°\", \"SKT\"]\n",
    "\n",
    "# ì´ìŠˆë³„ íŒŒì¼ëª… í¬ë§·\n",
    "FILENAME_FMT = \"{issue}_í†µí•©_final_ì¶”ê°€ì±„ë„_ì „ì²˜ë¦¬ì™„ë£Œ.xlsx\"\n",
    "\n",
    "# ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…\n",
    "TEXT_COL = \"title_or_text\"\n",
    "\n",
    "# ì˜µì…˜: ì´ìŠˆë³„ 'ë‹¨ì–´ ë¹ˆë„ ìš”ì•½'ì„ ì¶”ê°€ ì €ì¥í• ì§€\n",
    "SAVE_WORD_FREQ = True\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ìµœì‹  ë ‰ì‹œì½˜ ì •ì˜\n",
    "# ------------------------------------------------------------\n",
    "LEXICON = {\n",
    "    \"Event\": [\n",
    "        \"ìœ ì¶œ\", \"ì •ë³´ìœ ì¶œ\", \"í•´í‚¹\", \"ì¹¨í•´\", \"ë³´ì•ˆì‚¬ê³ \", \"ì‚¬ì´ë²„ê³µê²©\", \"ëœì„¬ì›¨ì–´\",\n",
    "        \"í”¼ì‹±\", \"ìŠ¤ë¯¸ì‹±\", \"ë””ë„ìŠ¤\", \"ë°ì´í„°\", \"ì„œë²„ìœ ì¶œ\", \"ì‹¬ì¹´ë“œ\", \"ìœ ì‹¬\", \"usim\", \"sim\",\n",
    "        \"ê³„ì •íƒˆì·¨\", \"ì ‘ì†ì°¨ë‹¨\", \"DBìœ ì¶œ\", \"ê°œì¸ì •ë³´\", \"ê³ ê°ì •ë³´\", \"ë‚´ë¶€ììœ ì¶œ\"\n",
    "    ],\n",
    "    \"Cause\": [\n",
    "        \"ë¶€ì£¼ì˜\", \"ë³´ì•ˆ\", \"ì„œë²„\", \"ì¸ì¦\", \"ì‹œìŠ¤í…œ\", \"íŒ¨ì¹˜\", \"ê´€ë¦¬\", \"ì‹¤ìˆ˜\", \"ì˜¤ë¥˜\", \"ì·¨ì•½ì \",\n",
    "        \"ë°©í™”ë²½\", \"ë°±ë„ì–´\", \"ë¯¸í¡\", \"ì¸ì ì˜¤ë¥˜\", \"ì •ì±…ë¶€ì¬\", \"ë‚´ë¶€í†µì œ\", \"ì™¸ì£¼\", \"ê´€ë¦¬ë¶€ì‹¤\", \"ì•”í˜¸í™”ë¯¸í¡\",\n",
    "        \"í´ë¼ìš°ë“œì„¤ì •\", \"ê¶Œí•œë‚¨ìš©\", \"ì ‘ê·¼í†µì œ\", \"ë³´ì•ˆíŒ¨ì¹˜\"\n",
    "    ],\n",
    "    \"Impact\": [\n",
    "        \"í”¼í•´\", \"ë¶ˆí¸\", \"í™˜ë¶ˆ\", \"ë³´ìƒ\", \"ê³¼ì§•ê¸ˆ\", \"ì†Œì†¡\", \"ì¡°ì‚¬\", \"ë²Œê¸ˆ\", \"ì§•ê³„\", \"ì‹ ë¢°í•˜ë½\",\n",
    "        \"ì£¼ê°€í•˜ë½\", \"ì´ë¯¸ì§€íƒ€ê²©\", \"í‰íŒí•˜ë½\", \"ì´ìš©ìì´íƒˆ\", \"ê°œì¸ì •ë³´ë…¸ì¶œ\", \"ì†í•´ë°°ìƒ\", \"í”¼í•´ì\", \"ë¶ˆì´ìµ\", \"ê³ ê°í”¼í•´\", \"ìœ„ì•½ê¸ˆ\"\n",
    "    ],\n",
    "    \"Reaction\": [\n",
    "        \"ë¶„ë…¸\", \"ë¶ˆì•ˆ\", \"ì¶©ê²©\", \"ì‚¬ê³¼\", \"ì±…ì„\", \"ë³´ìƒí•´ë¼\", \"í•´ëª…í•˜ë¼\", \"ë¬´ì±…ì„\", \"ì€í\", \"ë¶ˆë§¤\", \"ì‹ ë¢°\", \"í•˜ë½\",\n",
    "        \"ì‚­ì œìš”êµ¬\", \"í•´ì§€\", \"íƒˆí‡´\", \"ë¹„ë‚œ\", \"ë¶ˆì‹ \", \"ìš°ë ¤\", \"ëŒ€ì‘\", \"ì¡°ì¹˜\", \"ì¬ë°œë°©ì§€\", \"ë³´ì™„\", \"ê°ì‚¬\",\n",
    "        \"ê²½ìœ„íŒŒì•…\", \"ëŒ€ì±…\", \"ê³ ì†Œ\", \"ìˆ˜ì‚¬\", \"ê²€ì°°\", \"ê²½ì°°\", \"ë°©í†µìœ„ì¡°ì‚¬\", \"êµ­íšŒì§ˆì˜\", \"ì‚¬í‡´ìš”êµ¬\"\n",
    "    ],\n",
    "    \"Entity\": [\n",
    "        \"sk\", \"skt\", \"skí…”ë ˆì½¤\", \"ì—ìŠ¤ì¼€ì´\", \"kt\", \"ì¼€ì´í‹°\", \"ë¡¯ë°\", \"ë¡¯ë°ì¹´ë“œ\", \"gsë¦¬í…Œì¼\", \"gs\",\n",
    "        \"ë°©í†µìœ„\", \"pipc\", \"ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ\", \"ê³¼ê¸°ë¶€\", \"ì •ë¶€\", \"ê¸ˆìœµê°ë…ì›\", \"ê³ ê°\", \"ì´ìš©ì\", \"ì–¸ë¡ \",\n",
    "        \"ìœ ì˜ìƒ\", \"ì„í—Œë¬¸\", \"í™ì›ì‹\", \"ëŒ€í‘œì´ì‚¬\", \"ê´€ê³„ì\", \"ë³´ì•ˆíŒ€\", \"itê´€ë¦¬ì\", \"ë³¸ì‚¬\", \"ì§€ì \", \"í˜‘ë ¥ì—…ì²´\",\n",
    "        \"í†µì‹ ì‚¬\", \"ì¤‘êµ­\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ì˜ë¬¸Â·ìˆ«ì ì†Œë¬¸ìí™” ëŒ€ìƒ(í•œê¸€ì€ ì›í˜• ìœ ì§€)\n",
    "def _normalize_en(s: str) -> str:\n",
    "    return re.sub(r\"[A-Z]\", lambda m: m.group(0).lower(), s)\n",
    "\n",
    "# ë³µí•©ì–´ ê°ì§€ë¥¼ ìœ„í•´ ì›ë¬¸ë„ ì •ê·œí™” ë²„ì „ìœ¼ë¡œ ìœ ì§€\n",
    "def _normalize_text_for_substring(s: str) -> str:\n",
    "    # ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì¶•ì•½ì€ í•˜ì§€ ì•ŠìŒ: ê³¼ë„í•œ ì¶•ì•½ì€ ì˜¤íƒâ†‘\n",
    "    return _normalize_en(s)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ í˜•íƒœì†Œ ê¸°ë°˜ + ì„œë¸ŒìŠ¤íŠ¸ë§ ë³´ê°• ë§¤ì¹­ í•¨ìˆ˜\n",
    "# ------------------------------------------------------------\n",
    "def analyze_text(text: str):\n",
    "    \"\"\"\n",
    "    ë°˜í™˜: {cat: count, f\"{cat}_words\": \"a, b, c\", ...}\n",
    "    - í† í° ê¸°ë°˜(lemma/form) ì§‘í•©ê³¼ ì›ë¬¸ ì„œë¸ŒìŠ¤íŠ¸ë§ ê¸°ë°˜ ì´ì¤‘ ë§¤ì¹­\n",
    "    - ì¤‘ë³µ ë‹¨ì–´ ì œê±°(ì§‘í•©) í›„ ê°œìˆ˜ ì‚°ì •\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        base = {}\n",
    "        for cat in LEXICON.keys():\n",
    "            base[cat] = 0\n",
    "            base[f\"{cat}_words\"] = \"\"\n",
    "        return base\n",
    "\n",
    "    text = str(text)\n",
    "    text_norm = _normalize_text_for_substring(text)\n",
    "\n",
    "    # 1) í’ˆì‚¬ ê¸°ë°˜ í† í° ì„¸íŠ¸ êµ¬ì„± (lemma ìš°ì„ , form ë³´ì¡°)\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    token_strs = []\n",
    "    for t in tokens:\n",
    "        # lemmaê°€ ìˆìœ¼ë©´ lemma ì‚¬ìš©\n",
    "        surf = t.lemma if t.lemma else t.form\n",
    "        # ì˜ë¬¸ì€ ì†Œë¬¸ìí™”\n",
    "        token_strs.append(_normalize_en(surf))\n",
    "\n",
    "    token_set = set(token_strs)\n",
    "\n",
    "    # 2) ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹­\n",
    "    result = {}\n",
    "    for cat, vocab in LEXICON.items():\n",
    "        matched = set()\n",
    "\n",
    "        for w in vocab:\n",
    "            w_norm = _normalize_en(w)\n",
    "\n",
    "            # (a) í† í° ê¸°ë°˜ ë§¤ì¹­: í† í°ì— 'ì •í™•íˆ' ì¡´ì¬í•˜ê±°ë‚˜ í† í° ë‚´ ë¶€ë¶„ì¼ì¹˜\n",
    "            #    - ì˜ì–´ ì•½ì–´(usim/sim) ë“± ë³€í˜• ëŒ€ì‘ ìœ„í•´ ë¶€ë¶„ì¼ì¹˜ë„ í—ˆìš©\n",
    "            hit_token = any((w_norm == tok) or (w_norm in tok) for tok in token_set)\n",
    "\n",
    "            # (b) ì›ë¬¸ ì„œë¸ŒìŠ¤íŠ¸ë§ ë§¤ì¹­: ë³µí•©ì–´(ì˜ˆ: ê°œì¸ì •ë³´ë…¸ì¶œ) í¬ì°©\n",
    "            hit_sub = (w in text) or (w_norm in text_norm)\n",
    "\n",
    "            if hit_token or hit_sub:\n",
    "                matched.add(w)\n",
    "\n",
    "        result[cat] = len(matched)\n",
    "        result[f\"{cat}_words\"] = \", \".join(sorted(matched)) if matched else \"\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4ï¸âƒ£ ì•ˆì „ ì—‘ì…€ ì €ì¥ì„ ìœ„í•œ datetime tz ì²˜ë¦¬\n",
    "# ------------------------------------------------------------\n",
    "def drop_timezone_inplace(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Excel ì €ì¥ í˜¸í™˜ì„ ìœ„í•´ tz-aware datetime -> tz-naive ë³€í™˜\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            try:\n",
    "                if hasattr(df[col].dt, \"tz\"):\n",
    "                    df[col] = df[col].dt.tz_localize(None)\n",
    "            except Exception:\n",
    "                # ì˜ëª»ëœ ê°’ì´ ì„ì—¬ìˆëŠ” ê²½ìš° coerce í›„ ì²˜ë¦¬\n",
    "                try:\n",
    "                    s = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "                    df[col] = s.dt.tz_localize(None)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5ï¸âƒ£ ì‹¤í–‰: ì´ìŠˆë³„ ì²˜ë¦¬\n",
    "# ------------------------------------------------------------\n",
    "summary = []  # ì œê±° ê±´ìˆ˜ ìš”ì•½\n",
    "# ì´ìŠˆë³„/ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ ë¹ˆë„ ì§‘ê³„ìš©\n",
    "issue_cat_wordfreq = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "for issue in ISSUES:\n",
    "    file_path = os.path.join(BASE_PATH, FILENAME_FMT.format(issue=issue))\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“‚ {issue} ì²˜ë¦¬ ì¤‘... â†’ {file_path}\")\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    if TEXT_COL not in df.columns:\n",
    "        print(f\"âš ï¸ {TEXT_COL} ì»¬ëŸ¼ ì—†ìŒ: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    tqdm.pandas(desc=f\"{issue} í˜•íƒœì†Œ ë¶„ì„+ë¼ë²¨ ë§¤ì¹­ ì¤‘\")\n",
    "    lex_results = df[TEXT_COL].progress_apply(analyze_text)\n",
    "    lex_df = pd.DataFrame(list(lex_results))\n",
    "\n",
    "    # ì›ë³¸ + ë§¤ì¹­ ê²°ê³¼ ê²°í•©\n",
    "    df = pd.concat([df, lex_df], axis=1)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5-1) ë§¤ì¹­ ì—†ëŠ” í–‰ ì œê±°\n",
    "    # --------------------------------------------------------\n",
    "    cat_cols = list(LEXICON.keys())\n",
    "    df[\"match_sum\"] = df[cat_cols].sum(axis=1)\n",
    "    initial_count = len(df)\n",
    "    df_filtered = df[df[\"match_sum\"] > 0].copy()\n",
    "    removed_count = initial_count - len(df_filtered)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5-2) content_id ë¶€ì—¬\n",
    "    # --------------------------------------------------------\n",
    "    df_filtered = df_filtered.reset_index(drop=True)\n",
    "    df_filtered[\"content_id\"] = df_filtered.index + 1\n",
    "    # content_id ë§¨ ì•ìœ¼ë¡œ\n",
    "    cols = [\"content_id\"] + [c for c in df_filtered.columns if c != \"content_id\"]\n",
    "    df_filtered = df_filtered[cols]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5-3) ë‹¨ì–´ ë¹ˆë„ ìš”ì•½(ì˜µì…˜)\n",
    "    #   ê° ì¹´í…Œê³ ë¦¬ë³„ *_wordsì—ì„œ ë‹¨ì–´ ë¶„í•´í•˜ì—¬ ë¹ˆë„ í•©ì‚°\n",
    "    # --------------------------------------------------------\n",
    "    if SAVE_WORD_FREQ:\n",
    "        for cat in LEXICON.keys():\n",
    "            words_col = f\"{cat}_words\"\n",
    "            if words_col in df_filtered.columns:\n",
    "                for ws in df_filtered[words_col].fillna(\"\"):\n",
    "                    if not ws:\n",
    "                        continue\n",
    "                    for w in map(str.strip, ws.split(\",\")):\n",
    "                        if w:\n",
    "                            issue_cat_wordfreq[issue][cat][w] += 1\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5-4) ì—‘ì…€ ì €ì¥(íƒ€ì„ì¡´ ì œê±° í›„)\n",
    "    # --------------------------------------------------------\n",
    "    drop_timezone_inplace(df_filtered)\n",
    "\n",
    "    save_path = os.path.join(BASE_PATH, f\"{issue}_í˜•íƒœì†Œë¼ë²¨_ë¯¸ë§¤ì¹­ì œê±°ì™„ë£Œ.xlsx\")\n",
    "    df_filtered.to_excel(save_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5-5) ë¡œê·¸\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"âœ… {issue} ì™„ë£Œ:\")\n",
    "    print(f\"   ì „ì²´ ê¸°ì‚¬: {initial_count:,}ê±´\")\n",
    "    print(f\"   ë§¤ì¹­ëœ ê¸°ì‚¬: {len(df_filtered):,}ê±´\")\n",
    "    print(f\"   ì œê±°ëœ ê¸°ì‚¬: {removed_count:,}ê±´\")\n",
    "    print(f\"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}\")\n",
    "\n",
    "    summary.append({\n",
    "        \"ì´ìŠˆ\": issue,\n",
    "        \"ì „ì²´ê¸°ì‚¬\": initial_count,\n",
    "        \"ë§¤ì¹­ëœê¸°ì‚¬\": len(df_filtered),\n",
    "        \"ì œê±°ëœê¸°ì‚¬\": removed_count,\n",
    "        \"ë§¤ì¹­ë¹„ìœ¨(%)\": round(len(df_filtered) / initial_count * 100, 1)\n",
    "    })\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6ï¸âƒ£ ìš”ì•½ í…Œì´ë¸” ì¶œë ¥ ë° ì €ì¥\n",
    "# ------------------------------------------------------------\n",
    "summary_df = pd.DataFrame(summary).reset_index(drop=True)\n",
    "print(\"\\nğŸ“Š ë ‰ì‹œì½˜ ë§¤ì¹­ ìš”ì•½ (ì´ìŠˆë³„)\")\n",
    "try:\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "except Exception:\n",
    "    print(summary_df)\n",
    "\n",
    "summary_save = os.path.join(BASE_PATH, \"LEXICON_ë§¤ì¹­ìš”ì•½.xlsx\")\n",
    "summary_df.to_excel(summary_save, index=False, engine=\"openpyxl\")\n",
    "print(f\"\\nğŸ’¾ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_save}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7ï¸âƒ£ ì´ìŠˆë³„ ë‹¨ì–´ ë¹ˆë„ Top ì €ì¥(ì˜µì…˜)\n",
    "#    issueë³„ ì‹œíŠ¸ë¡œ, ì¹´í…Œê³ ë¦¬/ë‹¨ì–´/ë¹ˆë„ í…Œì´ë¸” ê¸°ë¡\n",
    "# ------------------------------------------------------------\n",
    "if SAVE_WORD_FREQ and issue_cat_wordfreq:\n",
    "    freq_xlsx = os.path.join(BASE_PATH, \"LEXICON_ë‹¨ì–´ë¹ˆë„ìš”ì•½.xlsx\")\n",
    "    with pd.ExcelWriter(freq_xlsx, engine=\"openpyxl\") as writer:\n",
    "        for issue, cat_map in issue_cat_wordfreq.items():\n",
    "            rows = []\n",
    "            for cat, counter in cat_map.items():\n",
    "                for word, cnt in counter.most_common():\n",
    "                    rows.append({\"ì¹´í…Œê³ ë¦¬\": cat, \"ë‹¨ì–´\": word, \"ë¹ˆë„\": cnt})\n",
    "            if rows:\n",
    "                df_freq = pd.DataFrame(rows).sort_values([\"ì¹´í…Œê³ ë¦¬\", \"ë¹ˆë„\"], ascending=[True, False])\n",
    "                df_freq.to_excel(writer, sheet_name=str(issue)[:31], index=False)\n",
    "    print(f\"ğŸ’¾ ë‹¨ì–´ ë¹ˆë„ ìš”ì•½ ì €ì¥: {freq_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>post_sentiment</th>\n",
       "      <th>post_s</th>\n",
       "      <th>cmt_s_mean</th>\n",
       "      <th>abs_sent_gap</th>\n",
       "      <th>aligned_cnt_3h</th>\n",
       "      <th>aligned_cnt_6h</th>\n",
       "      <th>aligned_cnt_9h</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>...</th>\n",
       "      <th>Event_words</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Cause_words</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Impact_words</th>\n",
       "      <th>Reaction</th>\n",
       "      <th>Reaction_words</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Entity_words</th>\n",
       "      <th>match_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-06 17:59:31</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>ê°œì¸ì •ë³´, ìœ ì¶œ, ì •ë³´ìœ ì¶œ, í•´í‚¹</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>gs, gsë¦¬í…Œì¼</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-06 18:57:34</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.783784</td>\n",
       "      <td>0.22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6297.0</td>\n",
       "      <td>...</td>\n",
       "      <td>í•´í‚¹</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>í”¼í•´</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>gs, gsë¦¬í…Œì¼, ê³ ê°</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-01-06 21:38:57</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>...</td>\n",
       "      <td>ê°œì¸ì •ë³´, ìœ ì¶œ, í•´í‚¹</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>gs, gsë¦¬í…Œì¼, ê³ ê°</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-06 22:40:32</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ê°œì¸ì •ë³´, ìœ ì¶œ, í•´í‚¹</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>gs, gsë¦¬í…Œì¼</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-06 22:53:49</td>\n",
       "      <td>ë¶€ì •</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ê°œì¸ì •ë³´, ìœ ì¶œ, í•´í‚¹</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>gs, gsë¦¬í…Œì¼</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   content_id            datetime post_sentiment  post_s  cmt_s_mean  \\\n",
       "0           1 2025-01-06 17:59:31             ë¶€ì •    -1.0    0.000000   \n",
       "1           2 2025-01-06 18:57:34             ë¶€ì •    -1.0   -0.783784   \n",
       "2           3 2025-01-06 21:38:57             ë¶€ì •    -1.0   -0.777778   \n",
       "3           4 2025-01-06 22:40:32             ë¶€ì •    -1.0    0.000000   \n",
       "4           5 2025-01-06 22:53:49             ë¶€ì •    -1.0    0.000000   \n",
       "\n",
       "   abs_sent_gap  aligned_cnt_3h  aligned_cnt_6h  aligned_cnt_9h  viewCount  \\\n",
       "0          1.00             0.0             0.0             0.0       33.0   \n",
       "1          0.22            18.0            21.0            22.0     6297.0   \n",
       "2          0.22             2.0             2.0             2.0     4276.0   \n",
       "3          1.00             0.0             0.0             0.0        NaN   \n",
       "4          1.00             0.0             0.0             0.0        NaN   \n",
       "\n",
       "   ...         Event_words  Cause  Cause_words Impact Impact_words Reaction  \\\n",
       "0  ...  ê°œì¸ì •ë³´, ìœ ì¶œ, ì •ë³´ìœ ì¶œ, í•´í‚¹      0          NaN      0          NaN        0   \n",
       "1  ...                  í•´í‚¹      0          NaN      1           í”¼í•´        0   \n",
       "2  ...        ê°œì¸ì •ë³´, ìœ ì¶œ, í•´í‚¹      0          NaN      0          NaN        0   \n",
       "3  ...        ê°œì¸ì •ë³´, ìœ ì¶œ, í•´í‚¹      0          NaN      0          NaN        0   \n",
       "4  ...        ê°œì¸ì •ë³´, ìœ ì¶œ, í•´í‚¹      0          NaN      0          NaN        0   \n",
       "\n",
       "  Reaction_words  Entity   Entity_words  match_sum  \n",
       "0            NaN       2      gs, gsë¦¬í…Œì¼          6  \n",
       "1            NaN       3  gs, gsë¦¬í…Œì¼, ê³ ê°          5  \n",
       "2            NaN       3  gs, gsë¦¬í…Œì¼, ê³ ê°          6  \n",
       "3            NaN       2      gs, gsë¦¬í…Œì¼          5  \n",
       "4            NaN       2      gs, gsë¦¬í…Œì¼          5  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\speec\\OneDrive\\Desktop\\PoC_v2\\í†µí•©ê²°ê³¼_ì¶”ê°€ì±„ë„\\final\\ì „ì²˜ë¦¬_1109\\GS_í˜•íƒœì†Œë¼ë²¨_ë¯¸ë§¤ì¹­ì œê±°ì™„ë£Œ.xlsx\"\n",
    "df_gs = pd.read_excel(file_path)\n",
    "df_gs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
