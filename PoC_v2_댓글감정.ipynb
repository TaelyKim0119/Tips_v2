{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c79ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece torch pandas tqdm konlpy scikit-learn -q\n",
    "!git clone https://github.com/park1200656/KnuSentiLex.git knusentilexdownload 2>/dev/null || echo \"KnuSentiLex already present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "댓글 감정 분석중: 100%|██████████| 6/6 [00:00<00:00, 23.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  video_id                text text_label  fused_score\n",
      "0       v1   역시 대단하다 또 유출이네 ㅋㅋ         부정    -0.637439\n",
      "1       v1  서비스 좋아요. 개선도 빨라졌고요         중립     0.078816\n",
      "2       v1         아 이건 좀 아닌 듯         중립    -0.001716\n",
      "3       v2        업데이트 나쁘지 않네요         중립    -0.000021\n",
      "4       v2          최악이다 진심 실망         부정    -0.697706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 1️⃣  댓글 데이터 (예: youtube_comments)\n",
    "# # ------------------------------------------------------------\n",
    "# # 반드시 'text' 컬럼이 있어야 합니다.\n",
    "# # 예시)\n",
    "# # youtube_comments = pd.read_csv(\"youtube_comments.csv\")\n",
    "# # columns = ['video_id','text','likeCount','published_at']\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 2️⃣  감정 분석 함수 (기존 코드에서 가져옴)\n",
    "# # ------------------------------------------------------------\n",
    "\n",
    "# def fused_sent_score(text: str) -> dict:\n",
    "#     \"\"\"KR-FinBERT + Kiwi + KnuSentiLex 간단 결합 버전\"\"\"\n",
    "#     lex = lexicon_sent_score(text)\n",
    "#     fin = finbert_sent_score(text)\n",
    "#     fused = 0.7 * fin[\"finbert_score\"] + 0.3 * lex[\"lex_score\"]\n",
    "\n",
    "#     if fused > 0.2:\n",
    "#         label = \"긍정\"\n",
    "#     elif fused < -0.2:\n",
    "#         label = \"부정\"\n",
    "#     else:\n",
    "#         label = \"중립\"\n",
    "\n",
    "#     return {\"text_label\": label, \"fused_score\": float(np.clip(fused, -1, 1))}\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 3️⃣  댓글별 감정 라벨링 수행\n",
    "# # ------------------------------------------------------------\n",
    "\n",
    "# def add_text_label(df, text_col=\"text\"):\n",
    "#     results = []\n",
    "#     for text in tqdm(df[text_col], desc=\"댓글 감정 분석중\"):\n",
    "#         try:\n",
    "#             result = fused_sent_score(str(text))\n",
    "#         except Exception as e:\n",
    "#             result = {\"text_label\": \"오류\", \"fused_score\": 0.0}\n",
    "#         results.append(result)\n",
    "\n",
    "#     res_df = pd.DataFrame(results)\n",
    "#     df = pd.concat([df.reset_index(drop=True), res_df], axis=1)\n",
    "#     return df\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 4️⃣  실행\n",
    "# # ------------------------------------------------------------\n",
    "# youtube_comments_labeled = add_text_label(youtube_comments)\n",
    "\n",
    "# # 결과 확인\n",
    "# print(youtube_comments_labeled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144af129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e229bf32229460b90338836aa53b453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\speec\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\speec\\.cache\\huggingface\\hub\\models--alsgyu--sentiment-analysis-fine-tuned-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef765cc88af40899fe6844007425b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc51fe5af094bb98feb54d5e9fcf647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef9e57cff01491e830b099e57b3f012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058142a88e5c4788b33c1ee371e8c1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec66febfe37543679ee49b036761f1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문장: 와, 진짜 천재시네요. 이렇게 완벽한 실수는 처음 봅니다.\n",
      "→ 감정: 긍정 (0.915)\n",
      "세부 점수: {'부정': 0.01, '중립': 0.076, '긍정': 0.915}\n",
      "\n",
      "문장: 그렇게 잘 될 줄 알았습니다. 다음번엔 노벨상이라도 타시겠어요.\n",
      "→ 감정: 긍정 (0.953)\n",
      "세부 점수: {'부정': 0.004, '중립': 0.043, '긍정': 0.953}\n",
      "\n",
      "문장: 역시 기대를 저버리지 않으시네요.\n",
      "→ 감정: 중립 (0.481)\n",
      "세부 점수: {'부정': 0.149, '중립': 0.481, '긍정': 0.37}\n",
      "\n",
      "문장: 작성자에 의해 삭제된 댓글입니다.\n",
      "→ 감정: 긍정 (0.514)\n",
      "세부 점수: {'부정': 0.194, '중립': 0.292, '긍정': 0.514}\n",
      "\n",
      "문장: 당하기만 하는 나라\n",
      "→ 감정: 부정 (0.594)\n",
      "세부 점수: {'부정': 0.594, '중립': 0.294, '긍정': 0.112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\speec\\AppData\\Local\\Temp\\ipykernel_25632\\2955265296.py:37: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  \"confidence\": round(float(probs[label_id]), 3),\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================\n",
    "# # 1️⃣ 라이브러리 설치\n",
    "# # ============================================================\n",
    "# # (colab 또는 로컬 환경에서 1회만 실행)\n",
    "# # !pip install transformers torch kiwipiepy --upgrade\n",
    "\n",
    "# # ============================================================\n",
    "# # 2️⃣ 라이브러리 임포트\n",
    "# # ============================================================\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "# from kiwipiepy import Kiwi\n",
    "\n",
    "# # ============================================================\n",
    "# # 3️⃣ 모델 및 토크나이저 불러오기\n",
    "# # ============================================================\n",
    "# MODEL_NAME = \"alsgyu/sentiment-analysis-fine-tuned-model\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# # ============================================================\n",
    "# # 4️⃣ 감정 예측 함수 정의\n",
    "# # ============================================================\n",
    "# def predict_sentiment(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#     outputs = model(**inputs)\n",
    "#     probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
    "#     label_id = torch.argmax(probs).item()\n",
    "\n",
    "#     # 모델 레이블 정의 (모델 카드에 따라 달라질 수 있음)\n",
    "#     labels = [\"부정\", \"중립\", \"긍정\"] if probs.shape[0] == 3 else [\"부정\", \"긍정\"]\n",
    "\n",
    "#     return {\n",
    "#         \"text\": text,\n",
    "#         \"label\": labels[label_id],\n",
    "#         \"confidence\": round(float(probs[label_id]), 3),\n",
    "#         \"scores\": {labels[i]: round(float(p), 3) for i, p in enumerate(probs)}\n",
    "#     }\n",
    "\n",
    "# # ============================================================\n",
    "# # 5️⃣ 테스트 문장\n",
    "# # ============================================================\n",
    "# samples = [\n",
    "#     \"와, 진짜 천재시네요. 이렇게 완벽한 실수는 처음 봅니다.\",\n",
    "#     \"그렇게 잘 될 줄 알았습니다. 다음번엔 노벨상이라도 타시겠어요.\",\n",
    "#     \"역시 기대를 저버리지 않으시네요.\",\n",
    "#     \"작성자에 의해 삭제된 댓글입니다.\",\n",
    "#     \"당하기만 하는 나라\"\n",
    "# ]\n",
    "\n",
    "# # ============================================================\n",
    "# # 6️⃣ 실행\n",
    "# # ============================================================\n",
    "# results = [predict_sentiment(text) for text in samples]\n",
    "\n",
    "# for r in results:\n",
    "#     print(f\"\\n문장: {r['text']}\")\n",
    "#     print(f\"→ 감정: {r['label']} ({r['confidence']})\")\n",
    "#     print(f\"세부 점수: {r['scores']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58418eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'knusentilexdownload'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/park1200656/KnuSentiLex.git knusentilexdownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2모델 비교 실험 \n",
    "HYBRID_MODEL_CANDIDATES = [\n",
    "    \"snunlp/KR-FinBERT\",\n",
    "    \"snunlp/KR-FinBert-Sentiment\",\n",
    "    \"beomi/KcELECTRA-base\"\n",
    "]\n",
    "ALS_MODEL_NAME = \"alsgyu/sentiment-analysis-fine-tuned-model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67b1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 파일 찾음: knusentilexdownload/SentiWord_Dict.txt\n",
      "[LOAD] KnuSentiLex entries: 14852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-FinBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] base model loaded: snunlp/KR-FinBERT -> ['부정', '긍정']\n",
      "[LOAD] als model loaded: alsgyu/sentiment-analysis-fine-tuned-model -> ['부정', '중립', '긍정']\n",
      "\n",
      "문장: 여휴 이러다가 조만간 모든국민 통신 마비되는건가 의심스럽다 어디로 가야하나\n",
      ">> HYBRID : 분노(강부정) (score=-0.428, lex=-2.0) base=긍정\n",
      ">> ALS     : 부정 (conf=0.6244485974311829) dist= {'부정': 0.624, '중립': 0.267, '긍정': 0.108}\n",
      "\n",
      "문장: 저거이 10년전에예정됐던겁니다.카카오만문제냐는식사론글로 저는 현장에서 대각제되었던거고 인증까지\n",
      ">> HYBRID : 중립 (score=0.13, lex=0.0) base=긍정\n",
      ">> ALS     : 긍정 (conf=0.5859249830245972) dist= {'부정': 0.146, '중립': 0.268, '긍정': 0.586}\n",
      "\n",
      "문장: 국내 거주중인 홍*인 친구 부정하고 입국을 막아라. 더넘어 미국인도 추방하고.. 정부 뭐하나? 실망.\n",
      ">> HYBRID : 분노(강부정) (score=-0.47, lex=-2.0) base=긍정\n",
      ">> ALS     : 부정 (conf=0.6572269797325134) dist= {'부정': 0.657, '중립': 0.227, '긍정': 0.116}\n",
      "\n",
      "문장: SK랑 LG도 보안 강화해라.\n",
      ">> HYBRID : 중립 (score=0.102, lex=0.0) base=긍정\n",
      ">> ALS     : 긍정 (conf=0.4922817349433899) dist= {'부정': 0.207, '중립': 0.3, '긍정': 0.492}\n",
      "\n",
      "문장: 이정현이 일 사회제재 하라고 부추기는거 같다\n",
      ">> HYBRID : 중립 (score=0.109, lex=0.0) base=긍정\n",
      ">> ALS     : 부정 (conf=0.45726925134658813) dist= {'부정': 0.457, '중립': 0.429, '긍정': 0.114}\n",
      "\n",
      "문장: 진짜 우리나라 통신요금은 받아먹을거 다 받아먹고 기술개발이나 유지보수는 뭐같이 안하는듯\n",
      ">> HYBRID : 중립 (score=0.025, lex=0.0) base=긍정\n",
      ">> ALS     : 부정 (conf=0.6891667246818542) dist= {'부정': 0.689, '중립': 0.24, '긍정': 0.071}\n",
      "\n",
      "문장: 민주 정부 시절에 해서 대박이 났네 ㅋㅋㅋㅋ\n",
      ">> HYBRID : 중립 (score=0.081, lex=0.0) base=긍정\n",
      ">> ALS     : 긍정 (conf=0.9809451103210449) dist= {'부정': 0.002, '중립': 0.017, '긍정': 0.981}\n",
      "\n",
      "문장: KT가 더 악질이다 SK 하수인 보는듯 해서 신경써야겠다는 생각을 진짜 누군말까도 안한거가?\n",
      ">> HYBRID : 중립 (score=0.104, lex=0.0) base=긍정\n",
      ">> ALS     : 부정 (conf=0.6472840905189514) dist= {'부정': 0.647, '중립': 0.3, '긍정': 0.053}\n",
      "\n",
      "문장: 시 당연히 정부에서도 아주 개판이군.\n",
      ">> HYBRID : 중립 (score=0.125, lex=0.0) base=긍정\n",
      ">> ALS     : 부정 (conf=0.7083052396774292) dist= {'부정': 0.708, '중립': 0.159, '긍정': 0.133}\n",
      "\n",
      "문장: 돈만 받아 쳐먹을욕지 보상해줄 생각도없는 개그지 같은 놈들\n",
      ">> HYBRID : 분노(강부정) (score=-0.471, lex=-2.0) base=긍정\n",
      ">> ALS     : 부정 (conf=0.6725367903709412) dist= {'부정': 0.673, '중립': 0.189, '긍정': 0.138}\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================\n",
    "# # KnuSentiLex 사전 + 하이브리드 감정분석 통합 코드\n",
    "# # ============================================================\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from kiwipiepy import Kiwi\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from typing import Dict, List, Tuple\n",
    "\n",
    "# # -----------------------------\n",
    "# # 설정\n",
    "# # -----------------------------\n",
    "# KNU_REPO_URL = \"https://github.com/park1200656/KnuSentiLex.git\"\n",
    "# # ── 위 URL은 예시이며 실제 경로는 저장소 구조 확인 필요\n",
    "# HYBRID_MODEL_CANDIDATES = [\n",
    "#     \"snunlp/KR-FinBERT\",\n",
    "#     \"snunlp/KR-FinBert-Sentiment\",\n",
    "#     \"beomi/KcELECTRA-base\"\n",
    "# ]\n",
    "# ALS_MODEL_NAME = \"alsgyu/sentiment-analysis-fine-tuned-model\"\n",
    "\n",
    "# # -----------------------------\n",
    "# # 1) 사전 다운로드 및 로드\n",
    "# # -----------------------------\n",
    "# def download_knu(path: str) -> None:\n",
    "#     os.system(f\"wget -O {path} {KNU_REPO_URL}\")\n",
    "\n",
    "# def load_knu_lexicon(path: str) -> Dict[str, float]:\n",
    "#     lex = {}\n",
    "#     # 파일이 탭 또는 공백 구분일 가능성\n",
    "#     df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"word\", \"polarity\"], encoding=\"utf-8\")\n",
    "#     for _, r in df.iterrows():\n",
    "#         word = str(r[\"word\"]).strip()\n",
    "#         try:\n",
    "#             score = float(r[\"polarity\"])\n",
    "#         except:\n",
    "#             continue\n",
    "#         lex[word] = score\n",
    "#     return lex\n",
    "\n",
    "# # 파일 경로 찾기 - 여러 경로 시도\n",
    "# possible_paths = [\n",
    "#     \"knusentilexdownload/SentiWord_Dict.txt\",\n",
    "#     \"knusentilexdownload/KnuSentiLex/SentiWord_Dict.txt\",\n",
    "#     \"KnuSentiLex_SentiWord_Dict.txt\"\n",
    "# ]\n",
    "\n",
    "# LEXICON_PATH = None\n",
    "# for path in possible_paths:\n",
    "#     if os.path.exists(path):\n",
    "#         LEXICON_PATH = path\n",
    "#         print(f\"✓ 파일 찾음: {path}\")\n",
    "#         break\n",
    "\n",
    "# if LEXICON_PATH is None:\n",
    "#     print(\"⚠ SentiWord_Dict.txt를 찾을 수 없습니다!\")\n",
    "#     print(f\"가능한 경로들:\")\n",
    "#     for p in possible_paths:\n",
    "#         print(f\"  - {p}\")\n",
    "#     # 기본 경로로 설정\n",
    "#     LEXICON_PATH = possible_paths[0]\n",
    "\n",
    "# LEXICON = load_knu_lexicon(LEXICON_PATH)\n",
    "# print(f\"[LOAD] KnuSentiLex entries: {len(LEXICON)}\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # 2) 형태소 분석기 초기화\n",
    "# # -----------------------------\n",
    "# kiwi = Kiwi()\n",
    "\n",
    "# # -----------------------------\n",
    "# # 3) 모델 로딩 함수\n",
    "# # -----------------------------\n",
    "# def load_classifier(model_name: str) -> Tuple[AutoTokenizer, AutoModelForSequenceClassification, List[str]]:\n",
    "#     tok = AutoTokenizer.from_pretrained(model_name)\n",
    "#     mdl = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#     with torch.no_grad():\n",
    "#         dummy = tok(\"테스트 문장\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#         out = mdl(**dummy)\n",
    "#     n = out.logits.shape[-1]\n",
    "#     if n == 2:\n",
    "#         labels = [\"부정\", \"긍정\"]\n",
    "#     elif n == 3:\n",
    "#         labels = [\"부정\", \"중립\", \"긍정\"]\n",
    "#     else:\n",
    "#         labels = [f\"class_{i}\" for i in range(n)]\n",
    "#     return tok, mdl, labels\n",
    "\n",
    "# def try_load_any(candidates: List[str]) -> Tuple[str, AutoTokenizer, AutoModelForSequenceClassification, List[str]]:\n",
    "#     for name in candidates:\n",
    "#         try:\n",
    "#             tok, mdl, lbls = load_classifier(name)\n",
    "#             print(f\"[LOAD] base model loaded: {name} -> {lbls}\")\n",
    "#             return name, tok, mdl, lbls\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] failed to load {name}: {e}\")\n",
    "#             continue\n",
    "#     raise RuntimeError(\"No base model could be loaded.\")\n",
    "\n",
    "# BASE_NAME, base_tok, base_mdl, base_labels = try_load_any(HYBRID_MODEL_CANDIDATES)\n",
    "# als_tok, als_mdl, als_labels = load_classifier(ALS_MODEL_NAME)\n",
    "# print(f\"[LOAD] als model loaded: {ALS_MODEL_NAME} -> {als_labels}\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # 4) Lexicon 점수 함수\n",
    "# # -----------------------------\n",
    "# def lexicon_score(text: str) -> float:\n",
    "#     tokens = [t.form for t in kiwi.tokenize(text)]\n",
    "#     return sum(LEXICON.get(tok, 0.0) for tok in tokens)\n",
    "\n",
    "# def sarcasm_flag(text: str, lscore: float) -> bool:\n",
    "#     tokens = [t.form for t in kiwi.tokenize(text)]\n",
    "#     positive_markers = {\"천재\", \"대단\", \"역시\", \"잘\", \"노벨상\"}\n",
    "#     if any(tok in positive_markers for tok in tokens) and lscore < -0.3:\n",
    "#         return True\n",
    "#     if text.count(\"...\") >= 2 or (\"?\" in text and \"!\" in text):\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# # -----------------------------\n",
    "# # 5) 예측 함수\n",
    "# # -----------------------------\n",
    "# @torch.no_grad()\n",
    "# def predict_hf(tok, mdl, labels, text: str) -> Tuple[str, float, Dict[str,float]]:\n",
    "#     inputs = tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#     out = mdl(**inputs)\n",
    "#     probs = torch.softmax(out.logits, dim=-1)[0].cpu().numpy().tolist()\n",
    "#     idx = int(torch.argmax(out.logits, dim=-1).item())\n",
    "#     return labels[idx], float(probs[idx]), {labels[i]: round(probs[i],3) for i in range(len(labels))}\n",
    "\n",
    "# def hybrid_predict(text: str, w_model: float = 0.7, w_lex: float = 0.3) -> Dict:\n",
    "#     lscore = lexicon_score(text)\n",
    "#     base_label, base_conf, base_dist = predict_hf(base_tok, base_mdl, base_labels, text)\n",
    "#     signed = (base_dist.get(\"긍정\",0) - base_dist.get(\"부정\",0))\n",
    "#     final_score = w_model * signed + w_lex * lscore\n",
    "#     if \"삭제된 댓글\" in text or \"부적절한 표현\" in text:\n",
    "#         label = \"기타\"\n",
    "#     elif sarcasm_flag(text, lscore):\n",
    "#         label = \"반어/비꼼\"\n",
    "#     else:\n",
    "#         if final_score > 0.2:\n",
    "#             label = \"긍정\"\n",
    "#         elif final_score < -0.2:\n",
    "#             label = \"분노(강부정)\" if lscore < -1.0 else \"부정\"\n",
    "#         else:\n",
    "#             label = \"중립\"\n",
    "#     return {\n",
    "#         \"label\": label,\n",
    "#         \"score\": round(final_score,3),\n",
    "#         \"lex_score\": round(lscore,3),\n",
    "#         \"base_label\": base_label,\n",
    "#         \"base_conf\": round(base_conf,3),\n",
    "#         \"base_dist\": base_dist\n",
    "#     }\n",
    "\n",
    "# # -----------------------------\n",
    "# # 6) 비교 실행\n",
    "# # -----------------------------\n",
    "# def compare_models(texts: List[str]) -> None:\n",
    "#     for t in texts:\n",
    "#         hy = hybrid_predict(t)\n",
    "#         als_label, als_conf, als_dist = predict_hf(als_tok, als_mdl, als_labels, t)\n",
    "#         print(\"\\n문장:\", t)\n",
    "#         print(\">> HYBRID :\", hy[\"label\"], f\"(score={hy['score']}, lex={hy['lex_score']})\", f\"base={hy['base_label']}\")\n",
    "#         print(\">> ALS     :\", als_label, f\"(conf={als_conf})\", \"dist=\", als_dist)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     sample_comments = [\n",
    "#         \"여휴 이러다가 조만간 모든국민 통신 마비되는건가 의심스럽다 어디로 가야하나\",\n",
    "#         \"저거이 10년전에예정됐던겁니다.카카오만문제냐는식사론글로 저는 현장에서 대각제되었던거고 인증까지\",\n",
    "#         \"국내 거주중인 홍*인 친구 부정하고 입국을 막아라. 더넘어 미국인도 추방하고.. 정부 뭐하나? 실망.\",\n",
    "#         \"SK랑 LG도 보안 강화해라.\",\n",
    "#         \"이정현이 일 사회제재 하라고 부추기는거 같다\",\n",
    "#         \"진짜 우리나라 통신요금은 받아먹을거 다 받아먹고 기술개발이나 유지보수는 뭐같이 안하는듯\",\n",
    "#         \"민주 정부 시절에 해서 대박이 났네 ㅋㅋㅋㅋ\",\n",
    "#         \"KT가 더 악질이다 SK 하수인 보는듯 해서 신경써야겠다는 생각을 진짜 누군말까도 안한거가?\",\n",
    "#         \"시 당연히 정부에서도 아주 개판이군.\",\n",
    "#         \"돈만 받아 쳐먹을욕지 보상해줄 생각도없는 개그지 같은 놈들\"\n",
    "#     ]\n",
    "#     compare_models(sample_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba76fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[파일 경로 확인]\n",
      "\n",
      "찾은 .txt 파일: 10개\n",
      "  - knusentilexdownload\\neg_pol_word.txt\n",
      "  - knusentilexdownload\\obj_unknown_pol_word.txt\n",
      "  - knusentilexdownload\\pos_pol_word.txt\n",
      "  - knusentilexdownload\\ReadMe.txt\n",
      "  - knusentilexdownload\\SentiWord_Dict.txt\n",
      "  - knusentilexdownload\\KnuSentiLex\\neg_pol_word.txt\n",
      "  - knusentilexdownload\\KnuSentiLex\\obj_unknown_pol_word.txt\n",
      "  - knusentilexdownload\\KnuSentiLex\\pos_pol_word.txt\n",
      "  - knusentilexdownload\\KnuSentiLex\\ReadMe.txt\n",
      "  - knusentilexdownload\\KnuSentiLex\\SentiWord_Dict.txt\n",
      "\n",
      "[주요 파일 존재 여부]\n",
      "✓ knusentilexdownload\\pos_pol_word.txt\n",
      "✓ knusentilexdownload\\neg_pol_word.txt\n",
      "✓ knusentilexdownload\\KnuSentiLex\\pos_pol_word.txt\n",
      "✓ knusentilexdownload\\KnuSentiLex\\neg_pol_word.txt\n"
     ]
    }
   ],
   "source": [
    "# ========== 파일 경로 확인 및 수정 ==========\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"[파일 경로 확인]\")\n",
    "base_path = \"knusentilexdownload\"\n",
    "\n",
    "# 다운로드된 파일 확인\n",
    "all_files = glob.glob(f\"{base_path}/**/*.txt\", recursive=True)\n",
    "print(f\"\\n찾은 .txt 파일: {len(all_files)}개\")\n",
    "for f in all_files[:10]:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# 주요 파일 확인\n",
    "key_files = [\n",
    "    os.path.join(base_path, \"pos_pol_word.txt\"),\n",
    "    os.path.join(base_path, \"neg_pol_word.txt\"),\n",
    "    os.path.join(base_path, \"KnuSentiLex\", \"pos_pol_word.txt\"),\n",
    "    os.path.join(base_path, \"KnuSentiLex\", \"neg_pol_word.txt\"),\n",
    "]\n",
    "\n",
    "print(\"\\n[주요 파일 존재 여부]\")\n",
    "for f in key_files:\n",
    "    exists = \"✓\" if os.path.exists(f) else \"✗\"\n",
    "    print(f\"{exists} {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bcbd3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNU] downloaded raw file from: https://raw.githubusercontent.com/park1200656/knu_senti_dict/master/SentiWord_Dict.txt\n",
      "[KNU] entries: 14,841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-FinBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] base: snunlp/KR-FinBERT -> ['부정', '긍정']\n",
      "[LOAD] als  : alsgyu/sentiment-analysis-fine-tuned-model -> ['부정', '중립', '긍정']\n",
      "                                  text HY_label  HY_score  HY_lex   HY_base ALS_label  ALS_conf                                ALS_dist\n",
      "      와, 진짜 천재시네요. 이렇게 완벽한 실수는 처음 봅니다.       중립    -0.092     0.0 부정(0.566)        긍정     0.915  {'부정': 0.01, '중립': 0.076, '긍정': 0.915}\n",
      "                      교체합시다. 너무 찝찝하잖아요       중립    -0.061     0.0 부정(0.544)        부정     0.775 {'부정': 0.775, '중립': 0.198, '긍정': 0.027}\n",
      "                    작성자에 의해 삭제된 댓글입니다.       기타    -0.080     0.0 부정(0.557)        긍정     0.514 {'부정': 0.194, '중립': 0.292, '긍정': 0.514}\n",
      "                          두번다시 케이티 안쓴다       중립    -0.054     0.0 부정(0.539)        긍정     0.393 {'부정': 0.382, '중립': 0.225, '긍정': 0.393}\n",
      "보이스피싱하고 사기꾼광고 이런문제들을 엄격하게 다룰수 있으면 좋겠다~       중립    -0.128     0.0 부정(0.591)        긍정     0.621 {'부정': 0.024, '중립': 0.355, '긍정': 0.621}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# KnuSentiLex 사전 + 하이브리드 감정분석 (교정/강화 버전)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# -----------------------------\n",
    "# 0) 설정\n",
    "# -----------------------------\n",
    "# A안: raw 파일 직접 받기 (repo knu_senti_dict에 raw 파일이 존재)\n",
    "RAW_TXT_URLS = [\n",
    "    \"https://raw.githubusercontent.com/park1200656/knu_senti_dict/master/SentiWord_Dict.txt\",\n",
    "    \"https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/SentiWord_Dict.txt\",  # 대체 경로 (없을 수도 있음)\n",
    "]\n",
    "LEXICON_LOCAL = \"SentiWord_Dict.txt\"\n",
    "\n",
    "# B안: git clone 경로\n",
    "GIT_REPO = \"https://github.com/park1200656/KnuSentiLex.git\"\n",
    "GIT_DIR  = \"KnuSentiLex\"\n",
    "\n",
    "# 분류 모델 후보\n",
    "BASE_CANDIDATES = [\n",
    "    \"snunlp/KR-FinBERT\",                    # 최우선 (없으면 다음)\n",
    "    \"nlpai-lab/kcelectra-base-kor-finetuned-nsmc\",  # 대체 1\n",
    "    \"beomi/KcELECTRA-base\"                  # 대체 2\n",
    "]\n",
    "ALS_MODEL = \"alsgyu/sentiment-analysis-fine-tuned-model\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) KNU 사전 확보: RAW → GIT 순서로 시도\n",
    "# -----------------------------\n",
    "def ensure_lexicon_file() -> str:\n",
    "    # 1) raw 다운로드 시도\n",
    "    for url in RAW_TXT_URLS:\n",
    "      try:\n",
    "        rc = subprocess.call([\"curl\", \"-L\", \"-o\", LEXICON_LOCAL, url], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        if rc == 0 and os.path.exists(LEXICON_LOCAL) and os.path.getsize(LEXICON_LOCAL) > 0:\n",
    "            print(f\"[KNU] downloaded raw file from: {url}\")\n",
    "            return LEXICON_LOCAL\n",
    "      except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) git clone 후 파일 찾기\n",
    "    if not os.path.exists(GIT_DIR):\n",
    "        subprocess.call([\"git\", \"clone\", \"--depth\", \"1\", GIT_REPO], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    # 후보 경로들\n",
    "    candidates = [\n",
    "        os.path.join(GIT_DIR, \"SentiWord_Dict.txt\"),\n",
    "        os.path.join(GIT_DIR, \"data\", \"SentiWord_Dict.txt\"),\n",
    "        os.path.join(GIT_DIR, \"KnuSentiLex-master\", \"SentiWord_Dict.txt\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p) and os.path.getsize(p) > 0:\n",
    "            print(f\"[KNU] found file: {p}\")\n",
    "            return p\n",
    "\n",
    "    # 실패 시 종료\n",
    "    raise FileNotFoundError(\"KNU SentiWord_Dict.txt not found. raw URL/Git clone 모두 실패\")\n",
    "\n",
    "def load_knu_lexicon(path: str) -> Dict[str, float]:\n",
    "    # TSV, 2열(word/emoticon, polarity). BOM·잡문자 대응\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=[\"word\", \"polarity\"],\n",
    "        encoding=\"utf-8\",\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"skip\"\n",
    "    )\n",
    "    # 숫자 변환, NaN drop\n",
    "    df[\"polarity\"] = pd.to_numeric(df[\"polarity\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"word\", \"polarity\"])\n",
    "    # 공백 정리\n",
    "    df[\"word\"] = df[\"word\"].astype(str).str.strip()\n",
    "    # 딕셔너리화\n",
    "    return dict(zip(df[\"word\"], df[\"polarity\"]))\n",
    "\n",
    "LEXICON_PATH = ensure_lexicon_file()\n",
    "LEXICON: Dict[str, float] = load_knu_lexicon(LEXICON_PATH)\n",
    "print(f\"[KNU] entries: {len(LEXICON):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 형태소 분석기\n",
    "# -----------------------------\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def normalize_token(tok: str) -> str:\n",
    "    # 필요 시 추가 규칙(소문자화, 특수문자 예외 처리 등)\n",
    "    return tok.strip()\n",
    "\n",
    "def lexicon_score(text: str) -> float:\n",
    "    # 이모티콘/표정 등 KNU에 그대로 있는 토큰은 공백 분할로도 잡힘\n",
    "    tokens = [t.form for t in kiwi.tokenize(text)]\n",
    "    score = 0.0\n",
    "    for t in tokens:\n",
    "        key = normalize_token(t)\n",
    "        if key in LEXICON:\n",
    "            score += float(LEXICON[key])\n",
    "    # 원문 그대로 매칭(이모티콘 등)을 위해 한 번 더 가벼운 스캔\n",
    "    for raw_piece in text.split():\n",
    "        key = normalize_token(raw_piece)\n",
    "        score += float(LEXICON.get(key, 0.0))\n",
    "    return score\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 분류기 로더\n",
    "# -----------------------------\n",
    "def load_classifier(name: str):\n",
    "    tok = AutoTokenizer.from_pretrained(name)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "    with torch.no_grad():\n",
    "        dummy = tok(\"테스트\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        n_cls = mdl(**dummy).logits.shape[-1]\n",
    "    if n_cls == 2:\n",
    "        labels = [\"부정\", \"긍정\"]\n",
    "    elif n_cls == 3:\n",
    "        labels = [\"부정\", \"중립\", \"긍정\"]\n",
    "    else:\n",
    "        labels = [f\"class_{i}\" for i in range(n_cls)]\n",
    "    return tok, mdl, labels\n",
    "\n",
    "def try_load_any(cands: List[str]):\n",
    "    last = None\n",
    "    for n in cands:\n",
    "        try:\n",
    "            tok, mdl, labels = load_classifier(n)\n",
    "            print(f\"[LOAD] base: {n} -> {labels}\")\n",
    "            return n, tok, mdl, labels\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"no base model found; last error: {last}\")\n",
    "\n",
    "BASE_NAME, base_tok, base_mdl, base_labels = try_load_any(BASE_CANDIDATES)\n",
    "als_tok, als_mdl, als_labels = load_classifier(ALS_MODEL)\n",
    "print(f\"[LOAD] als  : {ALS_MODEL} -> {als_labels}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 예측 + 하이브리드\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_logits(tok, mdl, labels, text: str):\n",
    "    x = tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    logits = mdl(**x).logits\n",
    "    probs = torch.softmax(logits, dim=-1)[0].cpu().numpy().tolist()\n",
    "    idx = int(torch.argmax(logits, dim=-1).item())\n",
    "    return labels[idx], float(probs[idx]), {labels[i]: float(p) for i, p in enumerate(probs)}\n",
    "\n",
    "def sarcasm_flag(text: str, lscore: float) -> bool:\n",
    "    # 간단 규칙: 칭찬표현+부정 lexicon, 과도한 구두점, 반문\n",
    "    pos_markers = {\"천재\",\"대단\",\"역시\",\"완벽\",\"잘했다\",\"축하\"}\n",
    "    has_pos = any(w in text for w in pos_markers)\n",
    "    punct = (text.count(\"...\") >= 2) or (\"?!\" in text)\n",
    "    return (has_pos and lscore < -0.3) or punct\n",
    "\n",
    "def signed_from_dist(dist: Dict[str, float]) -> float:\n",
    "    p_pos = dist.get(\"긍정\", 0.0)\n",
    "    p_neg = dist.get(\"부정\", 0.0)\n",
    "    p_neu = dist.get(\"중립\", 0.0)\n",
    "    signed = p_pos - p_neg\n",
    "    # 중립이 매우 크면 신호를 약화\n",
    "    if p_neu > max(p_pos, p_neg):\n",
    "        signed *= (1.0 - p_neu)\n",
    "    return signed\n",
    "\n",
    "def hybrid_predict(text: str, w_model=0.7, w_lex=0.3):\n",
    "    lscore = lexicon_score(text)\n",
    "    base_label, base_conf, base_dist = predict_logits(base_tok, base_mdl, base_labels, text)\n",
    "    signed = signed_from_dist(base_dist)\n",
    "    final = w_model * signed + w_lex * lscore\n",
    "\n",
    "    if \"삭제된 댓글\" in text or \"부적절한 표현\" in text:\n",
    "        label = \"기타\"\n",
    "    elif sarcasm_flag(text, lscore):\n",
    "        label = \"반어/비꼼\"\n",
    "    else:\n",
    "        if final > 0.2:\n",
    "            label = \"긍정\"\n",
    "        elif final < -0.2:\n",
    "            label = \"분노(강부정)\" if (lscore < -1.0 or base_dist.get(\"부정\",0) > 0.85) else \"부정\"\n",
    "        else:\n",
    "            label = \"중립\"\n",
    "\n",
    "    return {\n",
    "        \"hy_label\": label,\n",
    "        \"hy_score\": round(final, 3),\n",
    "        \"lex\": round(lscore, 3),\n",
    "        \"base_label\": base_label,\n",
    "        \"base_conf\": round(base_conf, 3),\n",
    "        \"base_dist\": {k: round(v, 3) for k,v in base_dist.items()}\n",
    "    }\n",
    "\n",
    "def compare_models(texts: List[str]):\n",
    "    rows = []\n",
    "    for t in texts:\n",
    "        hy = hybrid_predict(t)\n",
    "        als_label, als_conf, als_dist = predict_logits(als_tok, als_mdl, als_labels, t)\n",
    "        rows.append({\n",
    "            \"text\": t,\n",
    "            \"HY_label\": hy[\"hy_label\"],\n",
    "            \"HY_score\": hy[\"hy_score\"],\n",
    "            \"HY_lex\": hy[\"lex\"],\n",
    "            \"HY_base\": f\"{hy['base_label']}({hy['base_conf']})\",\n",
    "            \"ALS_label\": als_label,\n",
    "            \"ALS_conf\": round(als_conf,3),\n",
    "            \"ALS_dist\": {k: round(v,3) for k,v in als_dist.items()}\n",
    "        })\n",
    "    df = pd.DataFrame(rows, columns=[\"text\",\"HY_label\",\"HY_score\",\"HY_lex\",\"HY_base\",\"ALS_label\",\"ALS_conf\",\"ALS_dist\"])\n",
    "    pd.set_option(\"display.max_colwidth\", 180)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample = [\n",
    "        \"와, 진짜 천재시네요. 이렇게 완벽한 실수는 처음 봅니다.\",\n",
    "        \"교체합시다. 너무 찝찝하잖아요\",\n",
    "        \"작성자에 의해 삭제된 댓글입니다.\",\n",
    "        \"두번다시 케이티 안쓴다\",\n",
    "        \"보이스피싱하고 사기꾼광고 이런문제들을 엄격하게 다룰수 있으면 좋겠다~\"\n",
    "    ]\n",
    "    compare_models(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ 부정 렉시콘 확장\n",
    "# ------------------------------------------------------------\n",
    "negative_lexicon = [\n",
    "    # 삭제 관련\n",
    "    \"작성자에 의해 삭제\", \"삭제된 댓글\", \"클린봇\", \"운영규정 미준수\", \"부적절한 표현\", \"검열\", \"비속어\",\n",
    "    # 불만/위약금/기업 비판\n",
    "    \"위약금\", \"손해배상\", \"환불\", \"보상\", \"배신\", \"기만\", \"피해\", \"불매\", \"해지\", \"사기\",\n",
    "    # 기업/이슈 관련 비판\n",
    "    \"통신사\", \"3사\", \"악당\", \"도둑\", \"쓰레기\", \"한심\", \"어이없\", \"뻔뻔\", \"불공정\", \"망했다\",\n",
    "    # 욕설/비하\n",
    "    \"개같\", \"ㅈㄴ\", \"ㅂㅅ\", \"씨발\", \"병신\", \"ㅗ\", \"죽어라\", \"미친\", \"좆\", \"꺼져\",\n",
    "    # 조롱/풍자\n",
    "    \"ㅋㅋ\", \"ㅎㅎ\", \"참\", \"또\", \"역시\", \"하하\", \"재밌네\", \"대단하네\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ 부정 감정 판정 함수\n",
    "# ------------------------------------------------------------\n",
    "def sentiment_from_lexicon(text, lexicon):\n",
    "    text = str(text).lower()\n",
    "    # 부정어 포함 여부 감지\n",
    "    if any(word in text for word in lexicon):\n",
    "        return -1  # 부정\n",
    "    else:\n",
    "        return 0   # 중립\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ ALS용 평점 변환\n",
    "# ------------------------------------------------------------\n",
    "def map_sentiment_to_rating(sentiment):\n",
    "    # -1(부정)=1점, 0(중립)=3점\n",
    "    return 1 if sentiment == -1 else 3\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ 댓글 데이터 예시 (엑셀 불러오기 가능)\n",
    "# ------------------------------------------------------------\n",
    "# 실제 데이터 불러오기\n",
    "# df = pd.read_excel(\"C:/Users/speec/OneDrive/Desktop/PoC_v2/YouTube_결과/GS리테일_정보유출_comments_20251103_154830.xlsx\")\n",
    "\n",
    "# 샘플 예시\n",
    "df = pd.DataFrame({\n",
    "    \"user_id\": [1, 1, 2, 3],\n",
    "    \"item_id\": [10, 11, 10, 12],\n",
    "    \"comment\": [\n",
    "        \"작성자에 의해 삭제된 댓글입니다.\",\n",
    "        \"위약금이 너무 심해요\",\n",
    "        \"클린봇이 부적절한 표현을 감지한 댓글입니다.\",\n",
    "        \"서비스가 좋아요 감사합니다\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ 감정 점수 및 평점 부여\n",
    "# ------------------------------------------------------------\n",
    "df[\"sentiment\"] = df[\"comment\"].apply(lambda x: sentiment_from_lexicon(x, negative_lexicon))\n",
    "df[\"rating\"] = df[\"sentiment\"].apply(map_sentiment_to_rating)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ 결과 확인\n",
    "# ------------------------------------------------------------\n",
    "print(df[[\"user_id\", \"item_id\", \"comment\", \"sentiment\", \"rating\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879dcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
