{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c79ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece torch pandas tqdm konlpy scikit-learn -q\n",
    "!git clone https://github.com/park1200656/KnuSentiLex.git knusentilexdownload 2>/dev/null || echo \"KnuSentiLex already present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2291721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "댓글 감정 분석중: 100%|██████████| 6/6 [00:00<00:00, 23.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  video_id                text text_label  fused_score\n",
      "0       v1   역시 대단하다 또 유출이네 ㅋㅋ         부정    -0.637439\n",
      "1       v1  서비스 좋아요. 개선도 빨라졌고요         중립     0.078816\n",
      "2       v1         아 이건 좀 아닌 듯         중립    -0.001716\n",
      "3       v2        업데이트 나쁘지 않네요         중립    -0.000021\n",
      "4       v2          최악이다 진심 실망         부정    -0.697706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣  댓글 데이터 (예: youtube_comments)\n",
    "# ------------------------------------------------------------\n",
    "# 반드시 'text' 컬럼이 있어야 합니다.\n",
    "# 예시)\n",
    "# youtube_comments = pd.read_csv(\"youtube_comments.csv\")\n",
    "# columns = ['video_id','text','likeCount','published_at']\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣  감정 분석 함수 (기존 코드에서 가져옴)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def fused_sent_score(text: str) -> dict:\n",
    "    \"\"\"KR-FinBERT + Kiwi + KnuSentiLex 간단 결합 버전\"\"\"\n",
    "    lex = lexicon_sent_score(text)\n",
    "    fin = finbert_sent_score(text)\n",
    "    fused = 0.7 * fin[\"finbert_score\"] + 0.3 * lex[\"lex_score\"]\n",
    "\n",
    "    if fused > 0.2:\n",
    "        label = \"긍정\"\n",
    "    elif fused < -0.2:\n",
    "        label = \"부정\"\n",
    "    else:\n",
    "        label = \"중립\"\n",
    "\n",
    "    return {\"text_label\": label, \"fused_score\": float(np.clip(fused, -1, 1))}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣  댓글별 감정 라벨링 수행\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def add_text_label(df, text_col=\"text\"):\n",
    "    results = []\n",
    "    for text in tqdm(df[text_col], desc=\"댓글 감정 분석중\"):\n",
    "        try:\n",
    "            result = fused_sent_score(str(text))\n",
    "        except Exception as e:\n",
    "            result = {\"text_label\": \"오류\", \"fused_score\": 0.0}\n",
    "        results.append(result)\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "    df = pd.concat([df.reset_index(drop=True), res_df], axis=1)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣  실행\n",
    "# ------------------------------------------------------------\n",
    "youtube_comments_labeled = add_text_label(youtube_comments)\n",
    "\n",
    "# 결과 확인\n",
    "print(youtube_comments_labeled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144af129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e229bf32229460b90338836aa53b453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\speec\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\speec\\.cache\\huggingface\\hub\\models--alsgyu--sentiment-analysis-fine-tuned-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef765cc88af40899fe6844007425b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc51fe5af094bb98feb54d5e9fcf647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef9e57cff01491e830b099e57b3f012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058142a88e5c4788b33c1ee371e8c1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec66febfe37543679ee49b036761f1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문장: 와, 진짜 천재시네요. 이렇게 완벽한 실수는 처음 봅니다.\n",
      "→ 감정: 긍정 (0.915)\n",
      "세부 점수: {'부정': 0.01, '중립': 0.076, '긍정': 0.915}\n",
      "\n",
      "문장: 그렇게 잘 될 줄 알았습니다. 다음번엔 노벨상이라도 타시겠어요.\n",
      "→ 감정: 긍정 (0.953)\n",
      "세부 점수: {'부정': 0.004, '중립': 0.043, '긍정': 0.953}\n",
      "\n",
      "문장: 역시 기대를 저버리지 않으시네요.\n",
      "→ 감정: 중립 (0.481)\n",
      "세부 점수: {'부정': 0.149, '중립': 0.481, '긍정': 0.37}\n",
      "\n",
      "문장: 작성자에 의해 삭제된 댓글입니다.\n",
      "→ 감정: 긍정 (0.514)\n",
      "세부 점수: {'부정': 0.194, '중립': 0.292, '긍정': 0.514}\n",
      "\n",
      "문장: 당하기만 하는 나라\n",
      "→ 감정: 부정 (0.594)\n",
      "세부 점수: {'부정': 0.594, '중립': 0.294, '긍정': 0.112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\speec\\AppData\\Local\\Temp\\ipykernel_25632\\2955265296.py:37: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  \"confidence\": round(float(probs[label_id]), 3),\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1️⃣ 라이브러리 설치\n",
    "# ============================================================\n",
    "# (colab 또는 로컬 환경에서 1회만 실행)\n",
    "# !pip install transformers torch kiwipiepy --upgrade\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ 라이브러리 임포트\n",
    "# ============================================================\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ 모델 및 토크나이저 불러오기\n",
    "# ============================================================\n",
    "MODEL_NAME = \"alsgyu/sentiment-analysis-fine-tuned-model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ 감정 예측 함수 정의\n",
    "# ============================================================\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
    "    label_id = torch.argmax(probs).item()\n",
    "\n",
    "    # 모델 레이블 정의 (모델 카드에 따라 달라질 수 있음)\n",
    "    labels = [\"부정\", \"중립\", \"긍정\"] if probs.shape[0] == 3 else [\"부정\", \"긍정\"]\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"label\": labels[label_id],\n",
    "        \"confidence\": round(float(probs[label_id]), 3),\n",
    "        \"scores\": {labels[i]: round(float(p), 3) for i, p in enumerate(probs)}\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣ 테스트 문장\n",
    "# ============================================================\n",
    "samples = [\n",
    "    \"와, 진짜 천재시네요. 이렇게 완벽한 실수는 처음 봅니다.\",\n",
    "    \"그렇게 잘 될 줄 알았습니다. 다음번엔 노벨상이라도 타시겠어요.\",\n",
    "    \"역시 기대를 저버리지 않으시네요.\",\n",
    "    \"작성자에 의해 삭제된 댓글입니다.\",\n",
    "    \"당하기만 하는 나라\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 6️⃣ 실행\n",
    "# ============================================================\n",
    "results = [predict_sentiment(text) for text in samples]\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\n문장: {r['text']}\")\n",
    "    print(f\"→ 감정: {r['label']} ({r['confidence']})\")\n",
    "    print(f\"세부 점수: {r['scores']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58418eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'knusentilexdownload'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/park1200656/KnuSentiLex.git knusentilexdownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67b1d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KnuSentiLex_SentiWord_Dict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(LEXICON_PATH):\n\u001b[0;32m     45\u001b[0m     download_knu(LEXICON_PATH)\n\u001b[1;32m---> 47\u001b[0m LEXICON \u001b[38;5;241m=\u001b[39m load_knu_lexicon(LEXICON_PATH)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LOAD] KnuSentiLex entries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(LEXICON)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 2) 형태소 분석기 초기화\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m, in \u001b[0;36mload_knu_lexicon\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     31\u001b[0m lex \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 파일이 탭 또는 공백 구분일 가능성\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolarity\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     35\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\speec\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\speec\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\speec\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\speec\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\speec\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KnuSentiLex_SentiWord_Dict.txt'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# KnuSentiLex 사전 + 하이브리드 감정분석 통합 코드\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# -----------------------------\n",
    "# 설정\n",
    "# -----------------------------\n",
    "KNU_REPO_URL = \"https://github.com/park1200656/KnuSentiLex.git\"\n",
    "# ── 위 URL은 예시이며 실제 경로는 저장소 구조 확인 필요\n",
    "HYBRID_MODEL_CANDIDATES = [\n",
    "    \"snunlp/KR-FinBERT\",\n",
    "    \"snunlp/KR-FinBert-Sentiment\",\n",
    "    \"beomi/KcELECTRA-base\"\n",
    "]\n",
    "ALS_MODEL_NAME = \"alsgyu/sentiment-analysis-fine-tuned-model\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 사전 다운로드 및 로드\n",
    "# -----------------------------\n",
    "def download_knu(path: str) -> None:\n",
    "    os.system(f\"wget -O {path} {KNU_REPO_URL}\")\n",
    "\n",
    "def load_knu_lexicon(path: str) -> Dict[str, float]:\n",
    "    lex = {}\n",
    "    # 파일이 탭 또는 공백 구분일 가능성\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"word\", \"polarity\"], encoding=\"utf-8\")\n",
    "    for _, r in df.iterrows():\n",
    "        word = str(r[\"word\"]).strip()\n",
    "        try:\n",
    "            score = float(r[\"polarity\"])\n",
    "        except:\n",
    "            continue\n",
    "        lex[word] = score\n",
    "    return lex\n",
    "\n",
    "# 파일 경로 찾기 - 여러 경로 시도\n",
    "possible_paths = [\n",
    "    \"knusentilexdownload/SentiWord_Dict.txt\",\n",
    "    \"knusentilexdownload/KnuSentiLex/SentiWord_Dict.txt\",\n",
    "    \"KnuSentiLex_SentiWord_Dict.txt\"\n",
    "]\n",
    "\n",
    "LEXICON_PATH = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        LEXICON_PATH = path\n",
    "        print(f\"✓ 파일 찾음: {path}\")\n",
    "        break\n",
    "\n",
    "if LEXICON_PATH is None:\n",
    "    print(\"⚠ SentiWord_Dict.txt를 찾을 수 없습니다!\")\n",
    "    print(f\"가능한 경로들:\")\n",
    "    for p in possible_paths:\n",
    "        print(f\"  - {p}\")\n",
    "    # 기본 경로로 설정\n",
    "    LEXICON_PATH = possible_paths[0]\n",
    "\n",
    "LEXICON = load_knu_lexicon(LEXICON_PATH)\n",
    "print(f\"[LOAD] KnuSentiLex entries: {len(LEXICON)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 형태소 분석기 초기화\n",
    "# -----------------------------\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 모델 로딩 함수\n",
    "# -----------------------------\n",
    "def load_classifier(model_name: str) -> Tuple[AutoTokenizer, AutoModelForSequenceClassification, List[str]]:\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    with torch.no_grad():\n",
    "        dummy = tok(\"테스트 문장\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        out = mdl(**dummy)\n",
    "    n = out.logits.shape[-1]\n",
    "    if n == 2:\n",
    "        labels = [\"부정\", \"긍정\"]\n",
    "    elif n == 3:\n",
    "        labels = [\"부정\", \"중립\", \"긍정\"]\n",
    "    else:\n",
    "        labels = [f\"class_{i}\" for i in range(n)]\n",
    "    return tok, mdl, labels\n",
    "\n",
    "def try_load_any(candidates: List[str]) -> Tuple[str, AutoTokenizer, AutoModelForSequenceClassification, List[str]]:\n",
    "    for name in candidates:\n",
    "        try:\n",
    "            tok, mdl, lbls = load_classifier(name)\n",
    "            print(f\"[LOAD] base model loaded: {name} -> {lbls}\")\n",
    "            return name, tok, mdl, lbls\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to load {name}: {e}\")\n",
    "            continue\n",
    "    raise RuntimeError(\"No base model could be loaded.\")\n",
    "\n",
    "BASE_NAME, base_tok, base_mdl, base_labels = try_load_any(HYBRID_MODEL_CANDIDATES)\n",
    "als_tok, als_mdl, als_labels = load_classifier(ALS_MODEL_NAME)\n",
    "print(f\"[LOAD] als model loaded: {ALS_MODEL_NAME} -> {als_labels}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Lexicon 점수 함수\n",
    "# -----------------------------\n",
    "def lexicon_score(text: str) -> float:\n",
    "    tokens = [t.form for t in kiwi.tokenize(text)]\n",
    "    return sum(LEXICON.get(tok, 0.0) for tok in tokens)\n",
    "\n",
    "def sarcasm_flag(text: str, lscore: float) -> bool:\n",
    "    tokens = [t.form for t in kiwi.tokenize(text)]\n",
    "    positive_markers = {\"천재\", \"대단\", \"역시\", \"잘\", \"노벨상\"}\n",
    "    if any(tok in positive_markers for tok in tokens) and lscore < -0.3:\n",
    "        return True\n",
    "    if text.count(\"...\") >= 2 or (\"?\" in text and \"!\" in text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# -----------------------------\n",
    "# 5) 예측 함수\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_hf(tok, mdl, labels, text: str) -> Tuple[str, float, Dict[str,float]]:\n",
    "    inputs = tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    out = mdl(**inputs)\n",
    "    probs = torch.softmax(out.logits, dim=-1)[0].cpu().numpy().tolist()\n",
    "    idx = int(torch.argmax(out.logits, dim=-1).item())\n",
    "    return labels[idx], float(probs[idx]), {labels[i]: round(probs[i],3) for i in range(len(labels))}\n",
    "\n",
    "def hybrid_predict(text: str, w_model: float = 0.7, w_lex: float = 0.3) -> Dict:\n",
    "    lscore = lexicon_score(text)\n",
    "    base_label, base_conf, base_dist = predict_hf(base_tok, base_mdl, base_labels, text)\n",
    "    signed = (base_dist.get(\"긍정\",0) - base_dist.get(\"부정\",0))\n",
    "    final_score = w_model * signed + w_lex * lscore\n",
    "    if \"삭제된 댓글\" in text or \"부적절한 표현\" in text:\n",
    "        label = \"기타\"\n",
    "    elif sarcasm_flag(text, lscore):\n",
    "        label = \"반어/비꼼\"\n",
    "    else:\n",
    "        if final_score > 0.2:\n",
    "            label = \"긍정\"\n",
    "        elif final_score < -0.2:\n",
    "            label = \"분노(강부정)\" if lscore < -1.0 else \"부정\"\n",
    "        else:\n",
    "            label = \"중립\"\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"score\": round(final_score,3),\n",
    "        \"lex_score\": round(lscore,3),\n",
    "        \"base_label\": base_label,\n",
    "        \"base_conf\": round(base_conf,3),\n",
    "        \"base_dist\": base_dist\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 6) 비교 실행\n",
    "# -----------------------------\n",
    "def compare_models(texts: List[str]) -> None:\n",
    "    for t in texts:\n",
    "        hy = hybrid_predict(t)\n",
    "        als_label, als_conf, als_dist = predict_hf(als_tok, als_mdl, als_labels, t)\n",
    "        print(\"\\n문장:\", t)\n",
    "        print(\">> HYBRID :\", hy[\"label\"], f\"(score={hy['score']}, lex={hy['lex_score']})\", f\"base={hy['base_label']}\")\n",
    "        print(\">> ALS     :\", als_label, f\"(conf={als_conf})\", \"dist=\", als_dist)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_comments = [\n",
    "        \"와, 진짜 천재시네요. 이렇게 완벽한 실수는 처음 봅니다.\",\n",
    "        \"그렇게 잘 될 줄 알았습니다. 다음번엔 노벨상이라도 타시겠어요.\",\n",
    "        \"역시 기대를 저버리지 않으시네요.\"\n",
    "    ]\n",
    "    compare_models(sample_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba76fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 파일 경로 확인 및 수정 ==========\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"[파일 경로 확인]\")\n",
    "base_path = \"knusentilexdownload\"\n",
    "\n",
    "# 다운로드된 파일 확인\n",
    "all_files = glob.glob(f\"{base_path}/**/*.txt\", recursive=True)\n",
    "print(f\"\\n찾은 .txt 파일: {len(all_files)}개\")\n",
    "for f in all_files[:10]:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# 주요 파일 확인\n",
    "key_files = [\n",
    "    os.path.join(base_path, \"pos_pol_word.txt\"),\n",
    "    os.path.join(base_path, \"neg_pol_word.txt\"),\n",
    "    os.path.join(base_path, \"KnuSentiLex\", \"pos_pol_word.txt\"),\n",
    "    os.path.join(base_path, \"KnuSentiLex\", \"neg_pol_word.txt\"),\n",
    "]\n",
    "\n",
    "print(\"\\n[주요 파일 존재 여부]\")\n",
    "for f in key_files:\n",
    "    exists = \"✓\" if os.path.exists(f) else \"✗\"\n",
    "    print(f\"{exists} {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbd3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
